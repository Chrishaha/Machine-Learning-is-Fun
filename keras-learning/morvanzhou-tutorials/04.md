# 深度学习框架 Keras 学习（4）使用 keras 搭建的卷积神经网络对 mnist 数据集进行分类预测

### 导入需要使用到的 Python 库和类

```python
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten
from keras.optimizers import Adam

# 下载 mnist 数据集
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```
### 数据的预处理：标准化
+ 这是卷积神经网络对图像数据的要求
```python
X_train = X_train.reshape(-1, 1, 28, 28) / 255.
X_test = X_test.reshape(-1, 1, 28, 28) / 255.
y_train = np_utils.to_categorical(y_train, num_classes=10)
y_test = np_utils.to_categorical(y_test, num_classes=10)
```
### 搭建 CNN 卷积神经网络模型

结构：

1、卷积层、激活函数、池化层；

2、卷积层、激活函数、池化层；

3、拉平、全连接层、激活函数

4、全连接层、softmax 层。

```python
model = Sequential()

model.add(Convolution2D(
    batch_input_shape=(None, 1, 28, 28),
    filters=32,  # 设置 32 个滤波器，经过这一层以后，就生成了 32 张图片
    kernel_size=5,  # 卷积核是一个 5*5 的矩阵
    strides=1,
    padding='same',  # Padding method
    data_format='channels_first'
))

model.add(Activation('relu'))  # 对于 cnn 来说，偏爱使用 relu 作为激活函数
model.add(MaxPooling2D(
    pool_size=2,
    strides=2,
    padding='same',
    data_format='channels_first'
))

# 添加第 2 层卷积层和池化层
model.add(Convolution2D(
    64,
    5,
    strides=1,
    padding='same',
    data_format='channels_first'
))
model.add(Activation('relu'))
model.add(MaxPooling2D(
    2,
    2,
    'same',
    data_format='channels_first'
))

# 接下来送入全连接层
# 送入全连接层之前，要将数据拉平
model.add(Flatten())
model.add(Dense(1024))
model.add(Activation('relu'))

# 输出层也是一个全连接层
model.add(Dense(10))
model.add(Activation('softmax'))

# 设置 adam 优化器
adam = Adam(lr=1e-4)

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 开始训练，epochs=1 表示我只训练一轮
print('------ train ------')
model.fit(X_train, y_train, epochs=1, batch_size=64)
# 训练好模型以后就可以看看在测试数据集上的表现如何了
print('------ test ------')
loss, accuracy = model.evaluate(X_test, y_test)
print('loss:', loss)
print('accuracy:', accuracy)
```
+ 可以看到卷积神经网络对 mnist 数据集的分类效果比一般的全连接神经网络要好。
