{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA 线性判别分析公式推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "学习 LDA 其实我也没觉得它有什么很重要的应用，甚至我还听到了一种说法，觉得 LDA 是一个比较鸡肋的算法（这一点恐怕要打一个大大的问号），但是在学习 LDA 的过程中，可以巩固我们对常见数学工具的应用，因为其中用到的拉格朗日乘子法，定义的协方差矩阵、投影到向量的思想是常见而且基础的。\n",
    "\n",
    "## LDA 的思想\n",
    "\n",
    "与 PCA 不同，LDA 需要利用**类标信息**进行降维，是一种监督学习降维技术）。\n",
    "\n",
    "1、训练的时候，给定训练样例集，设法将样例投影到**一条直线**上，使得：\n",
    "+ 同类样本的投影点尽可能地接近；\n",
    "+ 异类样本的投影点尽可能地远离。\n",
    "\n",
    "要学习的就是这样一条直线；\n",
    "\n",
    "投影到一条直线上，直线是一维的，所以带求的参数是一个向量（特指列向量）；那么降维体现在哪里呢？**我们求出了这个直线向量 $w$ 以后，可以**归一化**，得到一个单位向量，然后去掉这个单位向量中分量比较小的量，只保留比较大的分量，这样就达到了降维的目的**。\n",
    "\n",
    "2、预测的时候：将待预测的样本投影到学习到的直线上，根据它的投影点的位置来判断它的类别。\n",
    "\n",
    "## LDA 的核心思想\n",
    "\n",
    "LDA 的核心思想：**投影变换后**（1）类间距离最大，（2）类内方差最小。\n",
    "\n",
    "## 定量描述 LDA 的核心思想\n",
    "\n",
    "类标在 LDA 算法执行过程中就发挥了重要作用：\n",
    "\n",
    "$$\n",
    "\\frac{均值向量投影以后的距离尽可能大（不同的类投影以后更加分散）}{投影以后的两个类标的点的方差之和尽可能小（同类的点投影以后更加集中）}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LDA公式推导\n",
    "\n",
    "（1）我们将原始的数据矩阵记为 $X$，对应的类标向量记为 $y$。\n",
    "（2）根据类标 $0$ 和 $1$ 将矩阵 $X$ 和分为两个部分。$X_0$ 对应的均值向量为 $\\mu_0$， $X_1$ 对应的均值向量为 $\\mu_1$（这里均值向量是**每个行向量加起来以后除以向量的个数**）；\n",
    "（3）投影以后类间距离最大：即不同类标的均值向量投影以后落在直线上的点间的距离最大。\n",
    "\n",
    "类标为 $0$ 的点的均值向量投影到直线上得到的实数：$w^T\\mu_0$；（$w^T$ 是  $1 \\times n$维， $\\mu_0$是 $n \\times 1 维，所以 $w^T\\mu_0$ 是一个实数）。\n",
    "\n",
    "类标为 $1$ 的点的均值向量投影到直线上得到的实数： $w^T\\mu_1$。\n",
    "\n",
    "它们之间的距离，即为“投影到直线以后的类内间距”：$(w^T\\mu_0-w^T\\mu_1)^2$。\n",
    "整理一下：\n",
    "$$\n",
    "\\begin {aligned}\n",
    "(w^T\\mu_0-w^T\\mu_1)^2 &= (w^T(\\mu_0-\\mu_1))^2 \\\\\n",
    "&=(w^T(\\mu_0-\\mu_1))((\\mu_0-\\mu_1)^Tw)\\\\\n",
    "&=w^T((\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T)w\n",
    "\\end {aligned}\n",
    "$$\n",
    "\n",
    "（4）“投影以后组内的方差之和尽量小”（这里的方差没有除以倍数）用数学表达出来就是：\n",
    "类标为 $0$ 的点投影到直线上的方差是：\n",
    "$$\n",
    "\\sum_{x \\in X_0}(w^Tx-w^T\\mu_0)^2\n",
    "$$\n",
    "类标为 $1$ 的点投影到直线上的方差是：\n",
    "$$\n",
    "\\sum_{x \\in X_1}(w^Tx-w^T\\mu_1)^2\n",
    "$$\n",
    "它们的和是：\n",
    "\n",
    "$$\n",
    "\\sum_{x \\in X_0}(w^Tx-w^T\\mu_0)^2 + \\sum_{x \\in X_1}(w^Tx-w^T\\mu_1)^2=\\sum_{x \\in X_0}w^T(x-\\mu_0)(x-\\mu_0)^Tw + \\sum_{x \\in X_1}w^T(x-\\mu_1)(x-\\mu_1)^Tw\n",
    "$$\n",
    "**写成这样以后，才能够很清晰地看到书上定义的协方差矩阵**。\n",
    "\n",
    "\n",
    "参考资料：\n",
    "1、周志华《机器学习》\n",
    "2、刘建平：线性判别分析 LDA 原理总结\n",
    "http://www.cnblogs.com/pinard/p/6244265.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
