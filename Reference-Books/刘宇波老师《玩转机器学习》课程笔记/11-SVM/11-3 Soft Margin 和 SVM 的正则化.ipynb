{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-3 Soft Margin 和 SVM 的正则化\n",
    "\n",
    "Soft Margin 是有容错能力的 SVM，之前我们讨论的 hard margin 的 SVM 都假设数据集线性可分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\rm min} \\frac{1}{2}||w||^2\\\\\n",
    "$$\n",
    "$$\n",
    "{\\rm s.t.}  y^{(i)}(w^Tx^{(i)}) \\geq 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么我们会引入 Soft Margin 呢？因为有可能“支撑向量”是离群点。\n",
    "\n",
    "于是我们就有下面的数学表示：\n",
    "$$\n",
    "{\\rm s.t.}\\quad y^{(i)}(w^Tx^{(i)}) \\geq 1 - \\zeta_i\n",
    "$$\n",
    "其中 $\\zeta_i \\geq 0$，这里 $\\zeta_i$ 不是一个固定的数，并且我们不可能让 $\\zeta_i$ 无限大，所以，我们在目标函数上限制\n",
    "\n",
    "$$\n",
    "{\\rm min}\\quad \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{m} \\zeta_{i}\\\\\n",
    "$$\n",
    "$C\\sum_{i=1}^{m} \\zeta_{i}$ 叫做正则化项，它是 $L_1$ 正则化，同样，我们可以定义 $L_2$ 正则\n",
    "如何理解正则化：当 $C$ 越来越大的时候，我们的目标函数是求最小值，于是我们就逼迫**正则化项**应该越来越小，即容错空间越来越小，即此时的 Margin 是越来越 hard 的。\n",
    "反之，当 $C$ 越来越小的时候，正则化项就被忽略，因此容错空间就可以越来越大，margin 内部的点就可以越来越多，即朝着 soft 的方向发展。\n",
    "特别注意：\n",
    "\n",
    "**scikit-learn 中的 C 就是在这个位置。**\n",
    "总结：引入 soft margin 的原因是，让 SVM 模型对训练数据集有一定的容错能力，这样模型就不会对那些离群点敏感，于是泛化能力就更强了。\n",
    "以上都是针对线性 SVM 的理论推导，即假设我们的训练数据集都是线性可分的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
