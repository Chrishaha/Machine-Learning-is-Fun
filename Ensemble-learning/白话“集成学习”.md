

# 白话“集成学习”



（看看是不是要分系列介绍完，因为文章内容较多。）



集成学习简而言之就是使用多个模型进行预测，它是一套框架，或者说策略，是一个相对比较抽象的概念，我们经常听到的“随机森林”、“AdaBoost”、“XGBoost”都是在集成学习的不同或相似的框架上建立起来的各种具体算法，是使用多个模型对数据进行预测的一种策略。

典型的理解“集成学习”的例子就是一句俗语“三个臭皮匠，赛过诸葛亮”，“三个臭皮匠”、“诸葛亮”分别可以看成是 $3$ 个表现不太好的模型，和 $1$ 个优秀的模型，这句话说明了 $3$ 个不太好的模型放在一起，它们的作用很可能会超过这个优秀的模型。可是，我们还听过一句话“一个和尚挑水喝，两个和尚没水喝”，模型多了，可能效果更差，这就涉及到，把这些模型放在一起，如何做决策的问题了，是听那 $3$ 个臭皮匠里最聪明的，还是让他们 $3$ 个投票决定呢？这都是集成算法框架要解决的事情。这个例子仅仅体现的是随机森林在考虑的事情，集成学习的内容远比这个例子考虑的事情和角度要多。

另外，在我们生活中做重大决策的时候，往往也会听取不同的人的意见。我们看的以竞赛为形式的综艺节目，邀请的专家评委也不止一位，他们来自各个不同的领域，同时参赛选手的得分还会参考观众评委的投票，这些都是集成学习思想的体现。

在机器学习任务中，使用多个类似但有差异的模型预测，往往会比只用一个模型预测的效果要好。因此集成学习是各种机器学习算法竞赛要想取得好成绩必须采用的算法框架，大名鼎鼎的 XGBoost 、随机森林，就是集成学习算法框架的具体实现。



| 框架     | 典型实现                  | 特点                                                         |
| -------- | ------------------------- | ------------------------------------------------------------ |
| Bagging  | 随机森林                  | 1、有放回随机采样构造差异；<br>2、只看一部分样本和一部分特征，在一定程度上避免了高纬度、大数据的计算；<br/>3、因为各个子模型没有依赖关系，因此各个子模型可以并行运算；<br/>4、包外估计，可以用于评估模型好坏；<br>5、分类问题，将各个子模型的预测结果投票决策，最多的胜出。回归问题，取平均；<br>6、Bagging 能够降低方差，思想其实很简单，在《数理统计》中，我们都学过，其实就是“越平均，越稳定”，也是大数定理的体现，这里就不展开了；<br/>7、能得到重要特征，如果一个特征被诸多子模型通过“基尼系数”或者“信息增益率”等选择到，说明这个特征是重要特征。 |
| Boosting | AdaBoost、GBDT（XGBoost） | 自适应提升、梯度提升                                         |
| Stacking |                           | 类似于神经网络一样的堆叠算法                                 |

+ scikit-learn 中关于集成学习的介绍：[官方文档中关于集成学习的介绍](https://scikit-learn.org/stable/modules/ensemble.html)。

## 一、随机森林（Random Forest）

### 随机森林在集成算法中的地位

+ 是 Bagging 思想的具体实现；
+ 随机森林使用 CART（分类回归树）作为子模型，叫“森林”可以说是非常形象了；
+ “随机”体现在两个方面，即“两个随机”：1、bootstrap，即随机抽样指定数据的样本，并且是有放回抽样，每个子模型看到的数据是有差异的；2、特征选择随机，即“随机”选择一部分特征，**再在这部分特征中通过启发式或者是贪心选择的方法，基于基尼系数（或者信息增益率、平方误差等指标）依次选择特征构建决策树，每个子模型看到的特征子集是有差异的**；

这里谈一谈“随机”带来的好处：

1、通过“两个随机”，即“样本抽取随机”和“特征选择随机”，对数据在横向和纵向都引入了随机性，带来了子模型的差异性， 各个子模型之间存在差异，是随机森林，甚至是集成学习框架喜欢的样子，试想如果各个子模型都差不多，例如政协会议的所有议员都是一种声音，集成的意义其实就不大了；

2、随机抽取部分样本和部分特征在一定程度上避免了算法在高维度和大数量级上的运算，横向上节约了时间，比起一棵决策树，针对所有训练样本利用所有特征构建决策树花的时间要少；

3、随机森林带来了“包外估计”，可以利用这部分样本，检验随机森林模型的泛化能力。

“包外估计”的内容在周志华的《机器学习》中有介绍，也可以在刘建平的这篇文章 [Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html) 中找到。

### Scikit-learn 提供的随机森林类的一些参数说明

由于随机森林是一棵又一棵的决策树，因此，随机森林很多的参数和决策树是一致的。

这一部分摘抄了刘建平的文章：[scikit-learn随机森林调参小结](https://www.cnblogs.com/pinard/p/6160412.html)

| 参数名                   | 默认值                         | 解释                                                         |
| ------------------------ | ------------------------------ | ------------------------------------------------------------ |
| **max_feature**          | auto，即特征数的平凡根         | 每个子模型随机抽取的最大特征数，即随机抽取一些列构建模型。   |
| **max_depth**            |                                | 决策树的最大深度，越深表示模型越复杂，越容易过拟合。         |
| **min_samples_split**    | 2                              | 内部结点再划分所需最小样本数，即如果当前结点的样本数小于这个值，这个结点就成为了叶子结点。 |
| **min_samples_leaf**     | 1                              | 1、叶子结点最少样本数；<br>2、这个值限制了叶子结点最少的样本数，如果某叶子结点数目小于样本数，则会和兄弟节点一起合并以后成为一个叶子结点（这个操作叫做“剪枝”）；<br/>3、可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 |
| min_weight_fraction_leaf | 0，即不考虑权重问题            | 1、叶子结点最小的样本权重和；<br>2、这个值限制了叶子结点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝；<br>3、一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 |
| max_leaf_nodes           | None，不限制最大的叶子节点数。 | 1、最大叶子结点数，限制最大叶子节点数，可以防止过拟合；<br>2、如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 |
| min_impurity_split       | 1e-7                           | 1、结点划分最小不纯度；<br>2、这个值限制了决策树的增长，如果某节点的不纯度（分类基于基尼系数，回归基于均方差）小于这个阈值，则该结点不再生成子结点，成为叶子结点 。一般不推荐改动默认值。 |



```
oob_score
```



### 其它随机森林

1、极其随机的森林（ExtraTrees Classifier）

[官方文档](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)。

+ 横向方面：不用 boostrap 这个方法了，用全部的数据；

+ 纵向方面：干脆不选最优特征了，从全部的特征中随机选一个特征，我想这就是它叫 extra 的原因吧。

2、孤立森林（Isolation Forest）

用于异常点（Outlier）检测。[官方文档](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest) 和下面这篇文章 [iForest （Isolation Forest）孤立森林 异常检测 入门篇](https://zhuanlan.zhihu.com/p/25040651) 都介绍了孤立森林的思想。这里我简单概括一下：

（1）首先随机采样，因为我们只是找异常点，所以不用一下子把全部数据加载到内存；

（2）随机选择一个特征，在这个特征上依次选择合适的阈值进行**二分**，如果一些数据点很快地落入一个叶子结点，那么这些数据点很可能是离群点，因为那些相对集中的数据点要**二分**很多次才会落入叶子结点。关于这一点官方文档上是这样描述的：

> Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.

## 二、AdaBoost





## 三、提升树、梯度提升树





## 四、梯度提升树中的明星 —— XGBoost 







