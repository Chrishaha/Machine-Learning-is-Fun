{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 刘宇波老师《k 近邻算法》8 小节课笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# k 近邻（kNN）算法（1）基础知识\n",
    "\n",
    "这一节简单介绍了 kNN 算法的基础知识和简单的实现，内容十分简单。\n",
    "\n",
    "## 什么是 kNN\n",
    "\n",
    "通过计算“待分类数据点”与“已有数据集”中的所有数据点的距离。取距离最小的前 $k$ 个点，根据“少数服从多数”的原则，将这个数据点划分为出现次数最多的那个类别。\n",
    "\n",
    "具体的过程大致如下：\n",
    "\n",
    "1. 计算待预测的数据点与训练数据集中所有已知点的**距离**，装入一个 List 容器中；\n",
    "2. 对上面的 List 容器中的数据从小到大排列，取出其中的 $k$ 个；\n",
    "3. 对其中的 $k$ 个进行投票，票数多的那个类别就是我们要预测的类别。\n",
    "\n",
    "## 代码基本实现\n",
    "\n",
    "### 第 1 步：计算待预测的数据点与训练数据集中所有已知点的**距离**，装入一个 List 容器中。\n",
    "\n",
    "```python\n",
    "from math import sqrt\n",
    "\n",
    "distances = []\n",
    "for x_train in X_train:\n",
    "    # X 是我们要预测的那个点，例如 X = np.array([8.093607318,3.365731514])\n",
    "    d = sqrt(np.sum((x_train - X)**2))\n",
    "    distances.append(d)\n",
    "distances\n",
    "```\n",
    "\n",
    "还可以这么写：\n",
    "\n",
    "```\n",
    "distances = [sqrt(np.sum((x_train - X)**2)) for x_train in X_train]\n",
    "```\n",
    "\n",
    "另外，NumPy 也提供了相应的方法用于计算距离：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "distances = [np.linalg.norm(x_train - X) for x_train in X_train]\n",
    "```\n",
    "\n",
    "### 第 2 步：对上面的 List 容器中的数据从小到大排列，取出其中的 $k$ 个。\n",
    "\n",
    "```python\n",
    "# 按照距离从小到大排列\n",
    "nearest = np.argsort(distances)\n",
    "# 使用列表生成式，得到指定索引的标签值，然后进行投票\n",
    "k = 6\n",
    "topK_y = [y_train[i] for i in nearest[:k]]\n",
    "```\n",
    "\n",
    "### 第 3 步：对其中的 $k$ 个进行投票，票数多的那个类别就是我们要预测的类别。\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "votes = Counter(topK_y)\n",
    "# votes 长这样：Counter({0: 1, 1: 5})，表示 0 有 1 个，1 有 5 个\n",
    "\n",
    "# votes.most_common(1) 的结果是 [(1, 5)]\n",
    "# votes.most_common(2) 的解雇是 [(1, 5), (0, 1)]\n",
    "# 所以我们对预测数据点 X 的预测结果就可以表示成如下\n",
    "predict = votes.most_common(1)[0][0]\n",
    "```\n",
    "---\n",
    "## 我的总结与思考\n",
    "\n",
    "+ 虽说 kNN 我们在初次接触的时候，告诉我们它是一个分类算法，但 kNN 也可以用于回归，算法的思想非常简单，不过通过 kNN 的学习，可以让我们了解到机器学习算法的一般解决思路。\n",
    "+ kNN 算法每一次的预测都要计算“预测数据点与训练数据集中的所有的点的距离”，计算量大。\n",
    "+ 可以认为，训练数据集就是 kNN 算法本身，如果我们要使用这个算法，就要保存所有的训练数据集（耗费的内存多）。这一点不同于以后我们要学习的线性回归、逻辑回归、SVM ，这些算法学习到的是参数，可以这么认为，训练结束以后，我们得到了模型（算法）的参数，训练数据集就可以丢掉了。\n",
    "  补充说明：这个例子就好像，我们推导数学公式一样，数学公式得到以后，我们以后再计算的时候套公式就可以了，我们不会像推导公式一样，去做一些重复的计算。\n",
    "+ kNN 中的 $k$ 是一个超参数，所谓超参数，就是在模型训练之前，就要训练好的参数\n",
    "\n",
    "---\n",
    "## 参考资料\n",
    "\n",
    "+ 可以在我的 Notebook 中查看所有的试验代码：[4-1 k 近邻算法基础.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-1%20k%20%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80.ipynb)\n",
    "\n",
    "---\n",
    "## 待补充的内容\n",
    "\n",
    "+ 自己实现的 kNN 算法的封装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# k 近邻（kNN）算法（2）在 scikit-learn 中使用 kNN\n",
    "\n",
    "这一节介绍了如何在 scikit-learn 中使用 kNN 算法。\n",
    "\n",
    "## 官方文档\n",
    "\n",
    "+ 在官方文档中可以看到 ：可以使用 `KDTree` 和 `BallTree`  这两种数据结构来找到最近邻。我们上一节也提到过，我们最原始的通过计算距离的方式实现的 kNN 算法它的计算量很大，我们的目标仅仅是要找到距离预测点最近的那几个点，但是我们却把预测点与所有样本点的距离都计算了一遍。为此， `KDTree` 和 `BallTree` 就是两种高效的数据结构，来提高对预测点的最近 `k` 个邻居的搜索。\n",
    "+ 除了超参数 `k` 以外，还可以指定超参数 `weights`。可选的参数值有 “uniform” 和 “distance”，“uniform” 的意思是每个近邻点的权重是一样的，通过“少数服从多数”，即“投票表决”的方式，选出类别最多的点作为待预测点的类别。如果使用 “distance” 作为 “weights” 的值，则表示距离越近的点，得到的权重值越大。\n",
    "+ [手敲一下这个官方网站的例子](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py)，[使用 kNN 算法预测鸢尾花数据集（官网例子，中文注释）](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/%E4%BD%BF%E7%94%A8%20kNN%20%E7%AE%97%E6%B3%95%E9%A2%84%E6%B5%8B%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88%E5%AE%98%E7%BD%91%E4%BE%8B%E5%AD%90%EF%BC%89.ipynb)\n",
    "\n",
    "\n",
    "## scikit-learn 的代码使用片段\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# k 近邻算法也可以用于分类\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# 这里的 n_neighbors 是一个超参数\n",
    "\n",
    "# k 近邻算法严重依赖训练数据集，可以认为训练数据集就是模型本身\n",
    "\n",
    "# 原始的\n",
    "raw_data_X = [[3.39353321, 2.33127338],\n",
    "              [3.11007348, 1.78153964],\n",
    "              [1.34380883, 3.36836095],\n",
    "              [3.58229404, 4.67917911],\n",
    "              [2.28036244, 2.86699026],\n",
    "              [7.42343694, 4.69652288],\n",
    "              [5.745052, 3.5339898],\n",
    "              [9.17216862, 2.51110105],\n",
    "              [7.79278348, 3.424088894],\n",
    "              [7.93982082, 0.79163723]]\n",
    "raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "X_train = np.array(raw_data_X)\n",
    "y_train = np.array(raw_data_y)\n",
    "\n",
    "kNN_classifier = KNeighborsClassifier(n_neighbors=6)\n",
    "kNN_classifier.fit(X_train, y_train)\n",
    "\n",
    "X = np.array([8.093607318, 3.365731514])\n",
    "# 注意：这里传给 predict 方法的 dimension 应该是 2，所以，要 reshape 一下，reshape(-1, 2) 因为这里数据的维度是 2，或者 reshape(1, -1)\n",
    "y_predict = kNN_classifier.predict(X.reshape(1, -1))\n",
    "print(y_predict[0])\n",
    "```\n",
    "\n",
    "## 我的总结与思考\n",
    "\n",
    "+ 查看官方文档\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "+ [ApacheCN 组织翻译的官方文档](http://sklearn.apachecn.org/cn/0.19.0/modules/neighbors.html)\n",
    "+ 可以在我的 Notebook 中查看所有的试验代码：[4-2 scikit-learn中的机器学习算法封装.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-2%20scikit-learn%E4%B8%AD%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%81%E8%A3%85.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# k 近邻（kNN）算法（3）训练数据集与测试数据集\n",
    "这一节介绍了为什么要在所有的训练数据集中分出一部分来用于测试。\n",
    "\n",
    "## 划分测试数据集的意义\n",
    "+ 如果训练好的模型没有测试就使用，我们就无从知道我们的模型是否有效。例如我们只做习题，没有模拟考试，就直接参加高考一样。\n",
    "+ 真实环境中的数据的 label 或者说 target 有时候很难得到，所以从训练数据集中拿出一部分做预测是一种选择。\n",
    "+ 给数据打上 label 或者说 target 有时工作量很大。\n",
    "\n",
    "## 划分数据集的一个简单的实现\n",
    "\n",
    "### 第 1 步：首先打乱数据，代码实现的时候，为了提高效率，我们不去移动数据，而选择打乱索引。\n",
    "\n",
    "```python\n",
    "# 为了复现，我们可以设置一个随机数的种子\n",
    "# np.random.seed(666)\n",
    "shuffle_indexes = np.random.permutation(len(X))\n",
    "```\n",
    "\n",
    "\n",
    "### 第 2 步：设置测试数据占全部数据的比例。\n",
    "```python\n",
    "# 分割出来的测试数据占未分割数据的比例\n",
    "test_radio = 0.2\n",
    "test_size = int(len(X) * test_radio)\n",
    "```\n",
    "\n",
    "### 第 3 步：最后分割得到训练数据集和测试数据集。\n",
    "\n",
    "```python\n",
    "test_indexes = shuffle_indexes[:test_size]\n",
    "train_indexes = shuffle_indexes[test_size:]\n",
    "\n",
    "X_train = X[train_indexes]\n",
    "y_train = y[train_indexes]\n",
    "\n",
    "X_test = X[test_indexes]\n",
    "y_test = y[test_indexes]\n",
    "```\n",
    "\n",
    "可以将以上的步骤封装成一个方法来使用，不过 scikit-learn 已经为我们实现了。\n",
    "\n",
    "## 使用 scikit-learn 中的 k 近邻算法完成训练-测试数据集的分割，并预测、计算准确度\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 666)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "+ 可以在我的 Notebook 中查看所有的试验代码：[4-3 训练数据集，测试数据集.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-3%20%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86.ipynb)​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k 近邻（kNN）算法（4）分类准确度 accuracy\n",
    "\n",
    "这一节介绍了评价分类好坏的标准之一：accuracy。\n",
    "\n",
    "## 什么是分类准确度 accuracy\n",
    "\n",
    "+ 评测模型在测试数据集上的效果，是一个量化的指标。\n",
    "+ accuracy 其实很简单，这是对于分类问题而言的一个评判标准。accuracy = 分类正确的个数 / 总的数据个数。\n",
    "+ 分类准确度涉及模型选择的问题。\n",
    "+ 对于数据有极度偏斜的数据集来说，accuracy 不是一个很好的评判标准，这一点，我们以后再说。\n",
    "\n",
    "我们使用 scikit-learn 为我们提供的手写数字识别的例子。\n",
    "## 例子：手写数字识别\n",
    "\n",
    "关键代码以及注释（完整请见：[ 4-4 分类准确度.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-4%20%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E5%BA%A6.ipynb)​\n",
    "）：\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# digit 手写数字数据集\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# 查看这个数据集的描述，从输出中可以找对这个数据集介绍的网页\n",
    "digits.DESCR\n",
    "# 查看这个数据集有哪些字段\n",
    "digits.keys()\n",
    "\n",
    "# 原始数据的特征矩阵\n",
    "X = digits.data\n",
    "# 原始数据的标签矩阵\n",
    "y = digits.target\n",
    "\n",
    "# 找特征矩阵的一行（即一个数据），看看到底是什么\n",
    "some_digits = X[666]\n",
    "some_digits_image = some_digits.reshape(8,8)\n",
    "# imshow 看看数据是怎么回事\n",
    "plt.imshow(some_digits_image, cmap = matplotlib.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "# 计算分类准确度的步骤\n",
    "# 1、训练数据集和测试数据集分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)\n",
    "# 2、使用训练数据集训练模型\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "# 3、使用测试数据集预测\n",
    "y_predict = knn_clf.predict(X_test)\n",
    "# 4、计算分类准确度，下面 3 种方法都可以计算\n",
    "accuracy1 = sum(y_predict == y_test) / len(y_test)\n",
    "# 也可以使用 scikit-learn 提供的 sklearn.metrics.accuracy_score 方法直接计算\n",
    "accuracy2 = accuracy_score(y_test, y_predict)\n",
    "# 如果不关心预测的值是啥，可以直接把测试数据集的特征矩阵和标签矩阵传进去\n",
    "accuracy3 = knn_clf.score(X_test, y_test)\n",
    "```\n",
    "---\n",
    "## 参考资料\n",
    "\n",
    "+ 可以在我的 Notebook 中查看所有的试验代码：[ 4-4 分类准确度.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-4%20%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E5%BA%A6.ipynb)​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k 近邻（kNN）算法（5）超参数\n",
    "这一节介绍了什么是机器学习算法的超参数。\n",
    "\n",
    "## 什么是超参数，什么是模型参数\n",
    "+ 超参数：在算法运行前需要确定的参数，kNN 算法中的 k 就是典型的超参数。\n",
    "+ 模型参数：在算法执行的过程中学习的参数，kNN 算法没有模型参数。\n",
    "+\n",
    "## 寻找好的超参数的方法\n",
    "+ 领域知识\n",
    "+ 经验数值\n",
    "+ 实验搜索：scikit-learn 就为我们提供了快捷的网格搜索的方式。\n",
    "\n",
    "## 实验：直接写一个 for 循环，通过在测试数据集上的分类准确度 accuracy 来判断哪个 k 最好\n",
    "下面的代码是一个很简单的实现了。\n",
    "\n",
    "```python\n",
    "best_score = 0.0\n",
    "best_k = -1\n",
    "for k in range(1,11):\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_clf.fit(X_train, y_train)\n",
    "    score = knn_clf.score(X_test, y_test)\n",
    "    if score > best_score:\n",
    "        best_k = k\n",
    "        best_score = score\n",
    "\n",
    "print('best_k',best_k)\n",
    "print('best_score',best_score)\n",
    "```\n",
    "\n",
    "## k 近邻算法的另一个超参数 weights\n",
    "\n",
    "+ 如果我们希望离预测点较近的点，有更多的说服力说明是属于这些更近的点的类别，即越近的点有越大的权重，可以使用超参数 weights = 'distance' 来设置。\n",
    "+ 如果设置了 weights = 'distance' 以后，如何定义距离，这又是一个可以讨论的话题，因此可以设置超参数 p 来修改距离的定义，这个 p 是 [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) 定义的 p ，当 $p = 1$ 的时候，距离的定义是对应坐标绝对值之和， 当 $p = 2$ 的时候，距离的定义就是我们的欧氏距离（对应坐标的差的平方之和，再开根号）。\n",
    "\n",
    "下面还是用粗暴的方法，写两个 for 循环来找到当 `weighs = 'distance'` 时候的最优超参数 `k` 和 `p`：\n",
    "\n",
    "```\n",
    "best_p = -1\n",
    "best_score = 0.0\n",
    "best_k = -1\n",
    "\n",
    "for k in range(1,11):\n",
    "    for p in range(1,6):\n",
    "        knn_clf = KNeighborsClassifier(n_neighbors=k, weights= 'distance', p = p)\n",
    "        knn_clf.fit(X_train, y_train)\n",
    "        score = knn_clf.score(X_test, y_test)\n",
    "        if score > best_score:\n",
    "            best_k = k\n",
    "            best_score = score\n",
    "            best_p = p\n",
    "\n",
    "print('best_p',best_p)\n",
    "print('best_k',best_k)\n",
    "print('best_score',best_score)\n",
    "```\n",
    "---\n",
    "## 参考资料\n",
    "\n",
    "+ Scikit-learn 官方文档：[Scikit-learn 官方文档中关于 sklearn.neighbors.KNeighborsClassifier 类的说明](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "+ 可以在我的 Notebook 中查看所有的试验代码：[4-5 超参数.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-5%20%E8%B6%85%E5%8F%82%E6%95%B0.ipynb)​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# k 近邻（kNN）算法（6）使用 scikit-learn 提供的网格搜索来找到最优超参数\"    ## 文章的标题\n",
    "\n",
    "这一节介绍了什么是机器学习算法的超参数。\n",
    "\n",
    "直接上代码说明问题：\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 网格搜索的第 1 步：超参数通过 json 字符串的方式传给 GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'weights':['uniform'],\n",
    "        'n_neighbors':[i for i in range(1, 11)]\n",
    "    },\n",
    "    {\n",
    "        'weights':['distance'],\n",
    "        'n_neighbors':[i for i in range(1, 11)],\n",
    "        'p':[i for i in range(1, 6)]\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=666)\n",
    "\n",
    "# 网格搜索的第 2 步：实例化 KNeighborsClassifier 的时候，不传任何超参数\n",
    "knn_clf = KNeighborsClassifier()\n",
    "# 网格搜索的第 3 步：实例化 GridSearchCV 类，传入模型和超参数\n",
    "grid_search = GridSearchCV(knn_clf, param_grid)\n",
    "# 网格搜索的第 4 步：使用 GridSearchCV 去训练\n",
    "# 网格搜搜的计算量大，耗时多\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 最佳 score\n",
    "grid_search.best_score_\n",
    "# 最佳的超参数\n",
    "grid_search.best_params_\n",
    "# {'n_neighbors': 3, 'p': 3, 'weights': 'distance'}\n",
    "\n",
    "# estimator 评估量，网格搜索最佳分类器的超参数的值\n",
    "grid_search.best_estimator_\n",
    "# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           # metric_params=None, n_jobs=1, n_neighbors=3, p=3,\n",
    "           # weights='distance')\n",
    "\n",
    "# 下面，我们就可以用那个最好的分类器，去运行我们的数据。\n",
    "knn_clf = grid_search.best_estimator_\n",
    "knn_clf.predict(X_test)\n",
    "knn_clf.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "更多 GridSearchCV 的参数的设置可以查看 Scikit-learn 官方文档：[Scikit-learn 官方文档中关于 sklearn.model_selection.GridSearchCV 类的说明](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)。\n",
    "\n",
    "```python\n",
    "# n_jobs = -1 将计算机所有的核都用于并行计算\n",
    "# verbose 在搜索的过程中进行一些输出，整数值越大，输出的信息越详细，通常选择 2\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "## 参考资料\n",
    "\n",
    "+ Scikit-learn 官方文档：[Scikit-learn 官方文档中关于 sklearn.model_selection.GridSearchCV 类的说明](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "+ 可以在我的 Notebook 中查看所有的试验代码：[ 4-6 网格搜索与k近邻算法中更多超参数.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-6%20%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E4%B8%8Ek%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E4%B8%AD%E6%9B%B4%E5%A4%9A%E8%B6%85%E5%8F%82%E6%95%B0.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# k 近邻（kNN）算法（7）Standardization\n",
    "\n",
    "这一节介绍了如何在机器学习中做 Standardization。\n",
    "\n",
    "要理解这一节的内容，我觉得主要把握以下几点，那就是：\n",
    "\n",
    "+ 因为涉及到距离的概念，不同特征使用的单位不同，量纲不同，很可能导致在计算距离的时候，最终的距离被单位较大或者量纲较大的特征所主导\n",
    "+ 那么如何解决这个问题呢：（1）将所有的特征缩放到一定区间（2）将所有的特征的均值处理成 0，方差处理成 1（这一点在本科的《概率论与数理统计》中有介绍），也叫数据的标准化。\n",
    "+ 将所有的特征缩放到一定区间，这种方式的一个弊端是，如果数据集中有离群点的话，效果就不会很好。那么这种方法适用于什么情况呢？所有的数据有明显的边界，例如学生的考试成绩。\n",
    "+ 一定要注意的一点是，**对于测试数据集，应该使用和训练数据集一样的均值、标准差（或者说最大值、最小值）进行处理，这一点一定要深刻体会它的必要性和合理性**。\n",
    "+ 掌握这部分知识最好的学习方法是查看官方文档，根据不同 Standardization 的定义，自己实现一遍（套用公式，代码都很短），在使用 scikit-learn 提供的 API 计算一次。\n",
    "+ 这一部分的知识，可以在官方文档的首页，在“数据的预处理”模块找到。\n",
    "\n",
    "---\n",
    "## 参考资料\n",
    "\n",
    "+ 官方文档英文：[4.3. Preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "+ ApacheCN 翻译的官方文档中文版：[4.3. 预处理数据](http://sklearn.apachecn.org/cn/0.19.0/modules/preprocessing.html#preprocessing)\n",
    "+ 可以在我的 Notebook 中查看所有的试验代码：[ 4-7 数据归一化.ipynb](https://nbviewer.jupyter.org/github/liweiwei1419/python-notes/blob/master/machine-learning-notes/%E7%AC%AC%204%20%E7%AB%A0%20%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%20kNN/4-7%20%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# k 近邻（kNN）算法（8）更多关于 k 近邻算法的思考\n",
    "\n",
    "这一节介绍了更多关于 k 近邻算法的思考。\n",
    "\n",
    "\n",
    "更多关于 k 近邻算法的思考\n",
    "\n",
    "+ k 近邻算法如何解决回归问题：找到最近的 k 个节点，取它们的平均值（或者加权平均）\n",
    "+ k 近邻算法的效率低下\n",
    "+ k 近邻算法高度数据相关\n",
    "+ k 近邻算法预测的结果不具有可解释性\n",
    "+ 维数灾难：所谓的维数灾难，就是“看似相近”的两个点之间的距离越来越大。（最好配图来说明）\n",
    "\n",
    "\n",
    "机器学习的流程回顾\n",
    "1、split\n",
    "2、保证数据的特征在同一个尺度下\n",
    "3、得到分类的准确度\n",
    "4、为了获得最好的超参数，可以使用网格搜索的方法\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
