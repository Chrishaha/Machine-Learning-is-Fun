{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 语料\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 将文本中的词语，转换成词频矩阵\n",
    "vectorizer = CountVectorizer()\n",
    "# 计算词语出现的频率\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个时候直接打印 X ，什么都看不到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# 获取词袋中所有文本关键词\n",
    "words = vectorizer.get_feature_names()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 查看词频结果\n",
    "'''\n",
    "同时在输出每个句子中包含特征词的个数。例如，第一句“This is the first document.”，它对应的\n",
    "词频为[0, 1, 1, 1, 0, 0, 1, 0, 1]，假设初始序号从1开始计数，则该词频表示存在第2个位置的单\n",
    "词“document”共1次、第3个位置的单词“first”共1次、第4个位置的单词“is”共1次、第9个位置的单词\n",
    "“this”共1词。所以，每个句子都会得到一个词频向量。\n",
    "'''\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 5)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# 第 1 个数字是文档的序号，第 2 个数字是在单词向量中的索引，第 3 个数字是词频\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "\n",
    "http://ilioner.github.io/2017/10/23/sklearn%E4%B8%ADCountVectorizer%E4%B8%8ETfidfTransformer%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "fromsklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    " \n",
    "\n",
    "\\1. CountVectorizer\n",
    "\n",
    " \n",
    "\n",
    "vectorizer=CountVectorizer(analyzer = \"word\",  \\\n",
    "\n",
    " \n",
    "\n",
    "                            tokenizer =None,    \\\n",
    "\n",
    " \n",
    "\n",
    "                            preprocessor =None, \\\n",
    "\n",
    " \n",
    "\n",
    "                            stop_words =None,   \\\n",
    "\n",
    " \n",
    "\n",
    "                            max_features =5000)\n",
    "\n",
    " \n",
    "\n",
    "将文本文档集合转换为token计数矩阵\n",
    "\n",
    "该实现使用scipy.sparse.csr_matrix生成计数的稀疏表示。\n",
    "\n",
    "如果您没有提供先验字典，而且也不使用某种特征选择的分析器，那么特征的数量将等于通过分析数据找到的词汇量。\n",
    "\n",
    " \n",
    "\n",
    "(1) analyzer字符串，{'单词'，'字符'，'char_wb'}或可调用\n",
    "\n",
    " \n",
    "\n",
    "特征是否应该由单词或字符n-gram组成。 选项“char_wb”仅从字边界内的文本创建字符n-gram; 单词边缘的n元素用空格填充。\n",
    "\n",
    " \n",
    "\n",
    "如果通过了可调用的函数，它将用来从原始未处理的输入中提取特征序列。\n",
    "\n",
    " \n",
    "\n",
    "(2) tokenizer可调用或无（默认）\n",
    "\n",
    " \n",
    "\n",
    "重写字符串标记化步骤，同时保留预处理和n-gram生成步骤。 只适用于分析仪=='单词'。\n",
    "\n",
    " \n",
    "\n",
    "(3) preprocessor 可调用或无（默认）\n",
    "\n",
    " \n",
    "\n",
    "重写预处理（字符串转换）阶段，同时保留标记化和n-gram生成步骤。\n",
    "\n",
    " \n",
    "\n",
    "(4) stop_words字符串{'english'}，列表或无（默认）\n",
    "\n",
    " \n",
    "\n",
    "如果使用“英语”，则使用英语的内置停用词表。\n",
    "\n",
    " \n",
    "\n",
    "如果列表中的列表被假定为包含停用词，则将从结果标记中删除所有这些词。 只适用于分析仪=='单词'。\n",
    "\n",
    " \n",
    "\n",
    "如果没有，则不会使用停用词。 可以将max_df设置为[0.7,1.0]范围内的一个值，以便根据术语内部语料文档的频率自动检测并过滤停用词。\n",
    "\n",
    " \n",
    "\n",
    "(5) ax_features int或None，default = None\n",
    "\n",
    " \n",
    "\n",
    "如果不是None，则建立一个词汇表，只考虑整个语料库中按词频排序的顶级max_features。\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\\2. TfidfVectorizer 部分参数\n",
    "\n",
    "TfidfVectorizer将原始文档集合转换为TF-IDF特征矩阵。等同于TvidfTransformer之后的CountVectorizer。\n",
    "\n",
    "（1） binary：默认为False，tf-idf中每个词的权值是tf*idf，如果binary设为True，所有出现的词的tf将置为1，TfidfVectorizer计算得到的tf与CountVectorizer得到的tf是一样的，就是词频，不是词频/该词所在文档的总词数。\n",
    "\n",
    "（2）norm：默认为'l2'，可设为'l1'或None，计算得到tf-idf值后，如果norm='l2'，则整行权值将归一化，即整行权值向量为单位向量，如果norm=None，则不会进行归一化。大多数情况下，使用归一化是有必要的。\n",
    "\n",
    "（3） use_idf：默认为True，权值是tf*idf，如果设为False，将不使用idf，就是只使用tf，相当于CountVectorizer了。\n",
    "\n",
    "（4） smooth_idf：idf平滑参数，默认为True，idf=ln((文档总数+1)/(包含该词的文档数+1))+1，如果设为False，idf=ln(文档总数/包含该词的文档数)+1\n",
    "\n",
    "（5） sublinear_tf：默认为False，如果设为True，则替换tf为1 +log(tf)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
