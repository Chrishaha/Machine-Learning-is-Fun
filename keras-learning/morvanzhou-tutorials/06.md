# 深度学习框架 Keras 学习（6）使用 Keras 搭建的 LSTM RNN 神经网络解决回归问题
这一节简单介绍了如何使用 Keras 搭建的 LSTM RNN 神经网络解决回归问题。

说实话，本节的内容我并不是很理解，只是手敲了一遍，具体内容可以直接看周莫烦的视频，在本文的参考资料里面有。

```python
使用 cos 函数曲线预测 sin 函数曲线。
# 导入需要使用的 Python 库和类
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import LSTM, TimeDistributed, Dense
from keras.optimizers import Adam

# 定义一些超参数
BATCH_START = 0
TIME_STEPS = 20
BATCH_SIZE = 50
INPUT_SIZE = 1
OUTPUT_SIZE = 1
CELL_SIZE = 20
LR = 0.006

# 生成序列
def get_batch():
    global BATCH_START, TIME_STEPS
    # 设置序列步长为 20，每次训练的 BATCH_SIZE 为 50
    xs = np.arange(BATCH_START, BATCH_START + TIME_STEPS * BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10 * np.pi)
    seq = np.sin(xs)
    res = np.cos(xs)
    BATCH_START += BATCH_SIZE
    plt.plot(xs[0, :], res[0, :], 'r', xs[0, :], seq[0, :], 'b--')
    plt.show()
    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]

# 输出数据大小由 CELL_SIZE 定义
model = Sequential()

model.add(LSTM(
    batch_size=BATCH_SIZE,
    input_shape=(TIME_STEPS, INPUT_SIZE),
    units=CELL_SIZE, # 输出数据的大小由 CELL_SIZE 定义
    return_sequences=True, # 因为每一个输入都对应一个输出，这个输出对下一个输入一起构成了下一个输出
    stateful=True # 每一个点的当前输出都受前面所有输出的影响，BATCH 之间的参数也需要记忆，故 stateful=True
))

# 所以添加输出层 LSTM 层的每一步都有输出，使用 TimeDistributed 函数。
model.add(TimeDistributed(Dense(OUTPUT_SIZE)))

# 定义优化器，激活模型
adam = Adam(LR)
model.compile(optimizer=adam, loss='mse')

print('------ train ------')

for step in range(501):
    X_batch, y_batch, xs = get_batch()
    cost = model.train_on_batch(X_batch, y_batch)
    pred = model.predict(X_batch, BATCH_SIZE)
    plt.plot(xs[0, :], y_batch[0].flatten(), 'r', xs[0, :], pred.flatten()[:TIME_STEPS], 'b--')
    plt.ylim((-1.2, 1.2))
    plt.draw()
    plt.pause(0.1)
    if step % 10 == 0:
        print('train cost:', cost)
```


## 参考资料

+ [2.8 RNN Regressor 循环神经网络](https://morvanzhou.github.io/tutorials/machine-learning/keras/2-5-RNN-LSTM-Regressor/)