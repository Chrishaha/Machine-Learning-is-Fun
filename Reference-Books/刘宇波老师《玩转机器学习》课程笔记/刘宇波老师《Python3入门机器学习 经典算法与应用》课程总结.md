# 刘宇波老师《[Python3入门机器学习 经典算法与应用](https://coding.imooc.com/class/169.html)》课程总结

恭喜你，学习完了整个课程。也感谢你对我的课程的支持：）

在这门课程中，你已经学会了相当多的机器学习算法，虽然在机器学习领域还有更多的内容值得深入探讨和学习，但是现在，你已经可以尝试在机器学习领域做一些实际的事情了。做什么事情？参加机器学习比赛是一个好的锻炼自己的方式。通常机器学习竞赛的数据和任务都是真实的，大家可以实打实地使用自己掌握的知识，来解决真实世界的问题：）

很多同学可能会怀疑，自己学习了这个课程以后，就能做机器学习的竞赛了吗？我的答案是：完全可以。机器学习领域的比赛和算法竞赛不同。对于算法竞赛，如果缺失关键的知识，对于大多数题目将一筹莫展，毫无思路。但机器学习不同，不管比赛的背景怎么变，要做的事情无非是预测识别。如果用机器学习的行话讲，就是解决一个回归问题或者分类问题。这个课程的所有方法，都是帮助你深入理解大多数经典的解决回归问题或者分类问题的算法的。都可以套用在机器学习的比赛中，完成比赛要求的内容的。

但是，也正是因为如此，机器学习竞赛在另一个维度上，比算法竞赛要难。因为，对于算法竞赛，一旦你彻底掌握了某个算法，就能完全解决某个或者某类问题。但是机器学习不同，使用不同的方法都可以解决同样的问题，问题的关键变成了：选择使用什么样的算法，使用怎样的流程，怎样预处理数据，等等等等，会得到更好的结果。这之中蕴含着无数的可能，说是无止境都不为过。换句话说，只学习这个课程的知识，可能很难在机器学习比赛中获得好的成绩。要想获得更好的成绩，需要深入学习。不过，我依然建议你学完这个课程就去真实的参加机器学习相关的比赛试试看，体会一下机器学习算法的应用，再继续深入学习：）

具体继续学习要学习什么，在这里简单列一下：

---

**任务**

首先，真实的应用机器学习，数据处理是非常重要的一部分。这部分内容是我们这个课程缺失的。这个课程假设我们已经获得了很好的数据，讲解机器学习算法的原理。但是实际情况通常是我们没有很完美的数据。怎样进行数据处理是非常重要的。

1）首先，最基本的，你需要掌握数据操作的方式。在这个课程中，我们主要使用的是numpy。但是对于真实的数据，在预处理阶段，使用 pandas 更方便。通常是先使用 pandas 预处理数据，之后转成 numpy 的 array 传给 sklearn 的（如果你是使用 sklearn 的话），所以，你需要掌握 pandas 的基本使用方法。

2）数据预处理方面一个重要的工作室**特征工程**。关于特征工程相关的内容，可以参考这个问答：<https://coding.imooc.com/learn/questiondetail/40722.html>

------

在具体算法角度，

3）在上面问答中，我提到了非监督学习。这个课程涉及的非监督学习并不多，只包含 PCA 。但是**实际在一些应用上，非监督学习可能会发挥很大的作用**。就以最常见的应用“降维”为例，也有很多不同的方法，可以用于处理不同类型的数据。

4）从监督学习的角度，从初级应用角度，这个课程没有介绍两个大方法：贝叶斯和神经网络。有兴趣可以找更多资料学习。

5）监督学习方面，从高级应用的角度，一个很大的部分是深度学习，这个课程完全不涉及。（不过，其实，一般在机器学习竞赛中，深度学习应用并不多）。另一类方法是集成学习。集成学习在这个课程的最后一章虽有介绍，但介绍的并不够深入，其中还有很多可以学习的内容。集成学习在机器学习竞赛中使用很广泛，建议深入学习。

6）特殊领域的特殊方法。在一些特殊领域，我们已经创建了很多经过验证效果不错的方法，需要你去学习专有领域。无论是图像，NLP（自然语言处理），甚至是医疗成像方向（或者更专一些的neuro imaging），都有专门的算法。搜索这些专门领域的教材或者论文资料，都会有介绍，在深入学习这些内容的时候，你就会看到这个课程中介绍的这些经典算法的基石性作用：）

7）还有一类数学建模的方法，不太会纳入一般机器学习的教程中，或许是因为太偏数学的原因。（也有一些教材在这方面做得很好，比如大名鼎鼎的PRML：<https://www.douban.com/link2/?url=https%3A%2F%2Fbook.douban.com%2Fsubject%2F2061116%2F&query=Pattern+Recognition+And+Machine+Learning&cat_id=1001&type=search&pos=0>

最典型的一类数学建模方法，就是随机过程中涉及的模型。比如 HMM 在语音识别中的应用，或者对时序数据的建模，等等等等。

------

在具体工具的掌握上，

8）除了上面说的 Pandas，还有一些专门的算法库也值得掌握。在深度学习领域诸如 Tensorflow 或者 Keras 这样的算法库就不用说了。由于在机器学习算法领域，集成学习效果很好，所以 xgboost 也值得掌握。

------

最后，学习材料上。

上面说的各个领域或者专题的材料，通常都有书籍或者教材支撑。只要搜索关键字就好。在这方面，我必须说，国内教程差国外教程一大截。无论是数量，可选择性，质量，各个方面。所以有能力阅读英文原版的话，请直接使用英文搜索相关材料。你的可选择性多了不止一个数量级。

如果只谈机器学习竞赛的材料，说实话，现在市面上，我没见过太好的专门介绍机器学习竞赛的材料。（其实在算法竞赛角度，我也没有见过太好的专门介绍的材料。我想主要是因为这种竞赛涵盖的内容太广了，很难用一本书讲清楚。）不过，和算法竞赛一样，最好的材料其实都是散落在互联网上的，需要你一点一点挖掘。这里，其实最大的资源库就是Kaggle本身。Kaggle不仅仅是一个竞赛网站，里面也蕴含了很多有价值的资料。

比如，今年，Kaggle刚刚上线了Kaggle Learn模块，使用Kaggle的数据，来一点点进行机器学习实战。可以参考：<https://www.kaggle.com/learn/overview>

其次，对于Kaggle的入门级比赛，有很好的Tutorial，请看这个页面的所有Tutorial，只针对一个泰坦尼克数据，使用不同方案解决，是很好的步入机器学习竞赛的第一步：<https://www.kaggle.com/c/titanic#tutorials>

Kaggle官方博客也有很多好的内容：<http://blog.kaggle.com/>

在你参加具体比赛的时候，Kaggle的论坛会是你的好帮手：<https://www.kaggle.com/discussion>

有人整理出了Kaggle比赛中很多问题的第一名的解决方案思路，其实这些内容都是引用上面Kaggle的官方博客或者论坛的内容：）

<http://ndres.me/kaggle-past-solutions/>

------

总之，可以学习的内容还有很多。但是我坚信，这个课程的基础内容学完以后，机器学习你就已经入门了。完全可以一边参加比赛增加经验，一边进行更深入的学习了：）

大家加油！





---

刘宇波老师给《特征工程和算法的关系》回答：http://coding.imooc.com/learn/questiondetail/40722.html

严格来讲，算法是算法，特征工程是特征工程。好的算法没有好的特征是毫无用武之地的。只用一个人的名字做特征，什么样的算法也无法判断这个人是否患某种疾病，就算给了身高体重也没用！而只有特征没有算法也是没有意义的，面对一片数据，无从下手。

这个课程相对比较强调算法。假设已经有了相对比较好的数据，看具体算法是如何使用的。也正是因为这个原因，这个课程涉及的非监督学习也不多。一定程度上，非监督学习都可以叫做特征工程：）不过以后有机会，我确实想讲一个更多强调特征工程的机器学习课程：）

整体而言，其实很多机器学习的书籍，对 general 的特征处理方式介绍的是 ok 的。只不过混杂在算法的介绍中，很多读者，尤其是第一遍学习，没有经过实战，会不在意。比如线性回归就可以作为特征选择的依据。这一点我在这个课程的线性回归可解释性一节有所提及；LASSO 也可以用于特征选择，我在这个课程中也会提及。决策树，随机森林，等等，都可以用于辨识已知特征的重要程度。另外诸如深度学习等方法，背后其实很大程度也是在解决特征的问题：）

另外，所有的非监督学习，或多或少都可以叫做在做特征工程（或者是数据的预处理）。这个课程中对PCA进行了详细的介绍，降维不仅仅是将高维特征映射到了低维空间，更能起到降噪的作用。我在这个课程中会提及。当然，非监督学习算法也是一个很广阔的领域，更多非监督学习算法，可以自己查阅其他资料。

再有，**很多 general 的特征工程的思想其实隐藏在统计学中**。系统学习一遍统计学，对发掘数据中的“秘密”很有帮助。毕竟，统计学的作用，就是要发掘数据中的“秘密”：）

最后，就是领域知识。对于特征工程来说，这是极端重要的。从某种程度上讲，各个专业领域研究的一个很重要的方向，都可以理解成在做特征工程。比如医学：研究到底哪些基因和哪些疾病相关；心理学，研究哪些大脑活动和哪些情绪相关；经济学，研究哪些社会现象，和经济表现结果相关，等等等等。我很早看过kaggle的一期冠军的采访，印象深刻。问他获得冠军的秘诀，他的秘诀就是，把精力放在了查找那个领域的文献上，从而使用了更靠谱的特征。（印象里那一期是和医学相关的数据）

---

刘宇波老师给《面对一个实际问题，我该选择非参数算法还是参数算法》的回答：https://coding.imooc.com/learn/questiondetail/43808.html

没有免费的午餐定理告诉我们的就是：在可能的情况下，我们应该尽可能去尝试用尽可能多地方案去解决。但是这个过程不是瞎试，我们需要了解各种方法的不同，用各种方法得到的结论逐渐组合优化我们的方法。这也是机器学习解决问题的思路和传统算法最大的不同。传统算法是解决一个固定问题，最终通常都存在一个最优解。而机器学习不同，这是一个不断探索，不断优化，不断认清自己的目标的过程，而不是得到一个固定答案的过程。

比如我在课程中曾经说过，**对于大部分数据，使用线性回归做一下尝试总是没有错的**，就是因为即使我们得到的预测结果不够好，但是我们通过线性回归的结果，依然可以看出来当前数据哪些特征和结果之间强相关；哪些弱相关。

再比如，通过对非参数学习算法的实验，我们可能会发现某些预测结果上有固定的偏差，这些偏差很有可能是因为已知数据集中某些特殊的 outlier 导致的。这也是非参数学习的特性所决定的：对特殊的数据敏感。尝试这些方法的过程中得到的这些结论，都可以帮助我们更加深刻的理解我们的数据。

另外，一些具体的领域，有相对认可度比较高的方法，比如图像领域的 CNN ；时序数据领域的 RNN 或者 LSTM（这两个模型都是深度学习的模型）；自然语言处理领域的 SVM 等；语音识别领域的隐马尔科夫模型等，这些已经被前人验证的可以在固定领域有较好表现的模型，在具体领域学习的时候需要注意。

最后，你说到多项式回归。其实多项式回归的本质是特征工程——也就是挖掘可能存在更好的特征的过程。这个过程其实在机器学习领域非常重要。具体可以参见这个问答：<https://coding.imooc.com/learn/questiondetail/40722.html>



但要注意，这个过程也不是有严格的先后顺序的，通常也是混杂在一起的。做好的特征使用算法去实验；在实验中根据表现的结果不同，反过来去优化特征工程的过程。一个好的机器学习工程师，并不一定可以非常好的解决问题（因为机器学习领域很多要解决的问题，实在是太复杂了，甚至很有可能使用现有的技术根本就无法解决好），更关键的是：能够解读机器学习算法的结果，理解这样的一个结果意味着什么，进而积极地去采取下一步可能的更优方案。拥有这样的素质，需要大量的经验积累。