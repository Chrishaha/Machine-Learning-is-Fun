{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 白话“朴素贝叶斯”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 朴素贝叶斯（Naive Bayers）是一种基于概率统计的**分类**方法，使用“朴素贝叶斯”可以预测概率；\n",
    "+ 应用：本文处理等；\n",
    "+ <b><font size='3' color='ff0000'>**朴素贝叶斯是最简单的概率有向图模型**</font></b>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么“朴素”？\n",
    "\n",
    "+ 引入<b><font size='3' color='ff0000'>**条件独立性假设**</font></b>，即在类别确定的前提下，数据的各个特征是独立的；\n",
    "\n",
    "说明：一般情况下，一个数据的各个特征是有关系的，例如一个人的身高和体重，但是朴素贝叶斯假设它们是独立的，虽然有悖于我们的认知，但是基于这个假设，在很多问题上，都能有很好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯定理（Bayes theorem）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本科《概率论与数理统计》的教材上，你会看到下面的公式：\n",
    "\n",
    "$$\n",
    "P(B_i|A) = \\frac{P(B_i)P(A|B_i)}{\\sum_{j=1}^{n}P(B_j)P(A|B_j)}\n",
    "$$\n",
    "\n",
    "但其实，你要你懂得条件概率与联合概率，用乘法公式就能够自己推出贝叶斯公式（上面右边式子中，分母是全概率公式，与本文无关，可以暂时忽略）。\n",
    "\n",
    "随机变量 $X$ 和 $Y$ 同时发生的联合概率分布也可以写成：\n",
    "\n",
    "$$\n",
    "P(X,Y) = P(X)P(Y|X)\n",
    "$$\n",
    "\n",
    "也可以写成：\n",
    "\n",
    "$$\n",
    "P(X,Y) = P(Y)P(X|Y)\n",
    "$$\n",
    "\n",
    "所以：\n",
    "$$\n",
    "P(X)P(Y|X)= P(Y)P(X|Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\cfrac{P(Y)P(X|Y)}{P(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯公式其实就是高中学习的乘法原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何使用贝叶斯公式做分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看公式：\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\cfrac{P(Y)P(X|Y)}{P(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 我们这里认为输入就是 $X$，一般而言是一个向量，输出 $Y$ 是一个类别变量，$P(Y|X)$ 是一个条件概率，我们这里称之为<b><font size='3' color='ff0000'>**后验概率**</font></b>，即<b><font size='3' color='ff0000'>知道了一些信息以后认为属于某一类的概率</font></b>；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## “条件独立假设”应用在计算条件概率的时候"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $P(X)$ 与 $Y$ 无关，可以认为是后验概率的归一化因子，这部分对于所有的数据都是一样的，所以我们可以不计算它；\n",
    "+ $P(Y)$ 是先验概率，即不知道任何信息的时候，我们对于一个类别变量的概率的估计，如果不是很能理解“先验概率”这个定义，可以参考本科《概率论与数理统计》教材相关章节，其实是很好理解的，相对于知道了某些信息的概率 $P(Y|X)$ （后验概率），什么都不知道的时候 $P(Y)$ 就是先验概率；\n",
    "\n",
    "+ $P(X|Y)$ 是条件概率了，即类别确定的时候，出现这个特征 $X$ 的概率，<b><font size='3' color='ff0000'>**条件独立性假设就应用在这里**</font></b>。\n",
    "\n",
    "一般 $X$ 是一个向量，这个向量的各个分量表示了一条数据的各个特征，下面，假设一个数据 $X$ 有 $3$ 个特征，即 $X = (x_1,x_2,x_3)$，那么在类别 $y$ 确定的条件下，数据 $X$ 的各个特征相互独立，用公式写出来就是：\n",
    "\n",
    "$$\n",
    "P(X|y) = P(x_1,x_2,x_3|y) = P(x_1|y) P(x_2|y) P(x_3|y) \n",
    "$$\n",
    "\n",
    "<b><font size='3' color='ff0000'>**为什么要引入这个假设呢？如果你真正使用贝叶斯定理去做一次垃圾短信分类或者垃圾邮件分类的任务，你就会发现 $P(x_1,x_2,x_3|y)$ 不好算，因为要判断 $3$ 个特征是否同时出现在一个数据里，甚至 $P(x_1,x_2,x_3|y)$ 这个值是很小的，引入条件独立性假设就是为了方便计算，因为单独的特征的条件概率 $P(x_1|y)$、$P(x_2|y)$、$P(x_3|y) $ 好算，只用做计数就可以了，就是这么简单。**</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以朴素贝叶斯分类法就是从训练数据集中，计算类别的先验分布 $P(Y)$ 和条件概率分布 $P(X|Y)$，然后就可以计算出联合概率分布  $P(X,Y)=P(Y)P(X|Y)$，<b><font size='3' color='ff0000'>**因为分母一样大，仅仅计算出贝叶斯公式分子的部分，然后比较分子的大小就可以了**</font></b>，联合概率大的，后验概率肯定也大。\n",
    "\n",
    "\n",
    "\n",
    "小结一下：朴素贝叶斯分类法，选择后验概率最大的类作为 $x$ 的输出，即每一个类别的概率都要算一遍，然后取概率最大的类作为输出：\n",
    "\n",
    "$$\n",
    "y = f(x) = \\arg \\max_{c_k} P(Y=c_k)\\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b><font size='3' color='ff0000'>**具体在工程中，会遇到两个问题。**</font></b>\n",
    "\n",
    "问题1：计算概率的乘积会使得很多小数连乘，最后得到的数值越来越小。  \n",
    "解决1：因此我们通常对上式取对数（以 e 为底，其它大于 1 的数为底都行），取对数的好处是：\n",
    "\n",
    "（1）对最终结果没有影响，原来大的数，取对数以后仍然大；  \n",
    "（2）取对数以后，连乘变连加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2：有的特征的取值，在某些特定的类别上并不出现，即会出现了概率为 $0$ 的情况。例如：在“非垃圾邮件中”，不会出现单词“国务院”（这只是我的推断，具体做垃圾邮件、短信分类的时候，就能够体会到我说的这个点）。 这时就要使用贝叶斯估计了：常常取一个 $\\lambda=1$ ，在分子分母同时做加法，使得分子分母都不为0。这一步也叫拉普拉斯修正（Laplacian correction）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、先验概率的贝叶斯估计\n",
    "\n",
    "$$\n",
    "P_{\\lambda} (Y=c_k) = \\frac{\\sum_{i=1}^{N}I(y_i=c_k)+\\lambda}{N+K\\lambda}\n",
    "$$\n",
    "\n",
    "2、条件概率的贝叶斯估计\n",
    "\n",
    "$$\n",
    "P_{\\lambda} (X^{(j)}=a_{jl}|Y=c_k) = \\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^{N} I(y_i=c_k)+S_j\\lambda}\n",
    "$$\n",
    "\n",
    "$S_j$ 表示第 $j$ 个特征 unique 以后得到的数，即第 $j$ 个特征的不同取值的个数。**满足非负性和归一性，因此是概率分布**。\n",
    "\n",
    "+ 二者的形式非常像。比起原来的数数字的方法而言，分子都加上 1，分母都加上的是这个类别的个数（去掉了重复）；\n",
    "+ 可以看看《统计学习方法》 P52例2，一下子就看懂了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面做概率估计的方法是极大似然估计（其实就是数数字）和贝叶斯估计（在数数字的基础上加上了拉普拉斯平滑）。<b><font size='3' color='0000ff'>**这部分内容可以参考李航的《统计学习方法》第 4 章有非常详细的讲解和例子**</font></b>。\n",
    "\n",
    "先验概率和条件概率就是简单的数数字就可以完成了，用样本概率估计总体概率的方法叫做**极大似然估计，即我们通过样本得到的概率值，就作为我们先验概率和条件概率的估计值**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## “后验概率最大化”符合机器学习的一般套路\n",
    "\n",
    "李航的《统计学习方法》第 4 章专门有一节就介绍了这个内容，具体的推导也很简单。\n",
    "\n",
    "> 朴素贝叶斯算法将实例分到“后验概率最大”的类中，这等价于期望风险最小化。\n",
    "\n",
    "后验概率最大化的含义：后验概率最大化等价于 0-1 损失函数时的期望风险最小化。定义 0-1 损失函数，期望是对联合概率分布 $P(X,Y)$ 取的。期望风险是损失函数对于联合概率分布的期望。理解这部分的叙述可以看这篇文章 [《朴素贝叶斯 后验概率最大化的含义》](https://blog.csdn.net/zhengjihao/article/details/78064121) 。\n",
    "\n",
    "这部分内容其实就是想说明“后验概率最大化”是合理的，符合机器学习的一般套路：找损失函数，使得损失最小（用专业的话讲就是“经验风险”，即训练数据集上的平均损失）的那个参数就是最好的模型，只不过朴素贝叶斯没有具体的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯方法实际上学习到的是生成数据的机制\n",
    "\n",
    "+ 生成模型和判别模型，其实我一开始根本搞不清楚，后来我发现，只要记住一点：<b><font size='3' color='ff0000'>**生成模型首先是概率模型，你首先要计算概率，但你不是直接计算后验概率，你是通过计算联合概率，比较联合概率的大小，这是生成模型**</font></b>；\n",
    "\n",
    "+ 我们的朴素贝叶斯分类法就是先计算联合概率分布，根据贝叶斯公式，联合概率分布大的后验概率也大；\n",
    "\n",
    "+ 典型的判别模型，同样也是计算概率的有：逻辑回归模型、决策树模型，它们压根没有联合概率什么事。\n",
    "\n",
    "关于判别模型和生成模型，可以参考李航《统计学习方法》第 12 章统计学习方法总结。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "1、李航《统计学习方法》第 4 章 朴素贝叶斯法\n",
    "\n",
    "2、刘建平《scikit-learn 朴素贝叶斯类库使用小结》\n",
    "https://www.cnblogs.com/pinard/p/6074222.html\n",
    "\n",
    "3、scikit-learn 官方文档\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "以下内容不一定采用：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为分母对所有的 $c_k$ 都是相同的，所以，只看分子就好了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、用于分类：朴素贝叶斯算法是使用统计学方法的一种分类方法。\n",
    "\n",
    "基于的假设是：对于已知的类别 ，朴素贝叶斯分类器在估计类条件概率的时候**假设特征之间条件独立（条件独立性）**。这个假设使得原本难以计算的条件联合概率转化为每个类别的条件概率的乘积，尤其是在特征很多的时候，就显得很简便。\n",
    "\n",
    "如果没有这个假设，就得遍历所有不同的条件组合，需要大量样本的支持。\n",
    "\n",
    "具体方法是：比较独立特征的条件概率（这句话不严谨），将概率最大的那个类别，作为该样本的类别。\n",
    "\n",
    "2、条件概率的定义，乘法原理就可以推导出贝叶斯公式。从贝叶斯公式中得出先验概率（两个先验概率，其中一个是标准化常量，因为它作为分母）、后验概率、条件概率、似然的定义。计算条件概率的时候，前提和结果可以反过来。\n",
    "\n",
    "3、频率学派和贝叶斯学派\n",
    "\n",
    "频率学派：认为概率是一个常数，是固定值，虽然未知但是是客观的。重点研究样本的分布。\n",
    "贝叶斯学派：认为概率是人的主观概念，代表了对事件发生的相信程度。重点研究参数的分布。\n",
    "\n",
    "4、朴素贝叶斯分类器是一种常见的且简单有效的贝叶斯分类算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数值型特征的处理：假设这个连续性特征呈现出高斯分布，先计算均值和标准差，然后代入公式计算得到条件概率，作为这个类别前提下特征的条件概率的估计值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高斯朴素贝叶斯（Gaussian naive Bayes）：假设每个标签的数据都服从简单的高斯分布。\n",
    "\n",
    "多项式朴素贝叶斯（Multinomial naive Bayes）：假设特征是由一个简单的多项式分布生成的。在词频统计这件事情上，使用多项式分布作为概率估计是合理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯算法优点\n",
    "1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。分类效果稳健，特别适合于大规模的数据集。\n",
    "2、支持增量式运算。即可以实时的对新增的样本进行训练。\n",
    "3、朴素贝叶斯对结果解释容易理解。\n",
    "4、天然地支持多分类任务。\n",
    "5、天生支持概率分类。\n",
    "6、对缺失的数据和噪声数据都不敏感\n",
    "\n",
    "\n",
    "## 朴素贝叶斯算法缺点\n",
    "1、由于使用了样本属性独立性的假设，所以**如果样本属性有关联时其效果不好**。特征之间的相互独立的假设在实际应用中往往是不成立的，在特征个数比较多或者特征之间相关性较大的时候，分类效果一般\n",
    "2、需要知道先验概率，而先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳；\n",
    "3、对输入数据的表达形式很敏感，应用在含有大量的数值特征的数据集的时候并不理想。\n",
    "三、朴素贝叶斯应用领域\n",
    "1、欺诈检测中使用较多\n",
    "2、一封电子邮件是否是垃圾邮件\n",
    "3、一篇文章应该分到科技、政治，还是体育类\n",
    "4、一段文字表达的是积极的情绪还是消极的情绪？\n",
    "5、人脸识别\n",
    "\n",
    "\n",
    "## 朴素贝叶斯算法的应用场景\n",
    "朴素贝叶斯分类器对数据有严格的假设，因此它的训练效果通常比复杂的模型差。\n",
    "1、训练和预测的速度非常快；\n",
    "2、直接使用概率预测\n",
    "3、通常很容易解释\n",
    "4、可调参数（如果有的话）非常少\n",
    "\n",
    "\n",
    "扩展阅读：半朴素贝叶斯分类器\n",
    "\n",
    "![在这里插入图片描述](https://img-blog.csdn.net/20181008154553273?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3X3Bvd2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例：文档分类\n",
    "\n",
    "1、下载文档\n",
    "\n",
    "```python\n",
    "from time import time\n",
    "from sklearn.datasets import load_files\n",
    "import os\n",
    "\n",
    "\n",
    "news_train = load_files('./datasets/mlcomp/379/train')\n",
    "print('end')\n",
    "print(\"summary: {0} documents in {1} categories.\".format(len(news_train.data), len(news_train.target_names)))\n",
    "print(\"done in {0} seconds\".format(time() - t))\n",
    "```\n",
    "2、文本特征预处理\n",
    "\n",
    "+ TF-IDF 用于评估一个词语对于一份文档的重要程度，我们利用 TF-IDF 这个工具，把一篇文档转换成一个向量。\n",
    "+ TF 表示词频，对于一份文档而言，词频是特定的词语在这篇文档里出现的次数除以这篇文档的词语总数。\n",
    "例如：某指定的一篇文档，一共有 1000 个词，其中“朴素贝叶斯”出现了 5 次，它的词频就是 0.005。\n",
    "\n",
    "+ IDF 表示一个词的**逆向文档频率指数（Inverse Document Frequency）**，由总文档数目，除以包含指定词语的文档的数目，再将商取对数得到，它表达的是词语的权重指数。\n",
    "例如：数据集一共有 10000 篇文档，“朴素贝叶斯”只出现在 10 篇文档中，则“朴素贝叶斯”的权重指数 ${\\rm IDF} = \\log \\frac{10000}{10} = 3$。\n",
    "\n",
    "+ 具体是这么做的：\n",
    "1、构建词典，如果词典有 10000 个词语，则每一篇文档就可以表示成一个 10000 维的向量，每一个分量就计算 TF-IDF 的值作为特征；\n",
    "2、一篇文档里词的数量相对于词典的数量是很少的，所以一篇文档转换成向量以后，绝大多数分量都为 0，因此得到的是一个稀疏的词向量矩阵。\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(encoding='latin-1')\n",
    "\n",
    "X_train = vectorizer.fit_transform(news_train.data)\n",
    "\n",
    "# 查看非零元素的个数 X_train[0].getnnz()\n",
    "```\n",
    "\n",
    "3、模型训练：送入朴素贝叶斯算法\n",
    "\n",
    "刘建平《scikit-learn 朴素贝叶斯类库使用小结》一文中说道：\n",
    "> 在 scikit-learn 中，一共有 3 个朴素贝叶斯的分类算法类。分别是 `GaussianNB`，`MultinomialNB` 和 `BernoulliNB`。其中 `GaussianNB` 就是先验为高斯分布的朴素贝叶斯，`MultinomialNB` 就是先验为多项式分布的朴素贝叶斯，而 `BernoulliNB` 就是先验为伯努利分布的朴素贝叶斯。\n",
    "\n",
    "文档分类问题中，文档的类别符合多项式分布，应该应该使用 `MultinomialNB`。\n",
    "\n",
    "**多项式分布**是指满足类别分布的实验，连续做 $n$ 次以后，每种类别出现的特定次数**组合**的概率分布情况。\n",
    "\n",
    "\n",
    "+ alpha 表示平滑参数，其值越小，越容易过拟合；值太大，容易欠拟合。\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# MultinomialNB 表示多项式朴素贝叶斯\n",
    "y_train = news_train.target\n",
    "clf = MultinomialNB(alpha=0.0001)\n",
    "clf.fit(X_train, y_train)\n",
    "train_score = clf.score(X_train, y_train)\n",
    "# 在训练数据集上的效果还是很不错的。\n",
    "```\n",
    "\n",
    "4、模型评价\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, pred,target_names=news_test.target_names))\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test,pred)\n",
    "print(cm)\n",
    "```\n",
    "\n",
    "将混淆矩阵画成热力图：\n",
    "```python\n",
    "# Show confusion matrix\n",
    "plt.figure(figsize=(8, 8), dpi=100)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "ax = plt.gca()                                  \n",
    "ax.spines['right'].set_color('none')            \n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_color('none')\n",
    "ax.spines['left'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('none')\n",
    "ax.yaxis.set_ticks_position('none')\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "plt.matshow(cm, fignum=1, cmap='gray')\n",
    "plt.colorbar();\n",
    "```\n",
    "\n",
    "补充：如果数据的特征值是连续的话，可以使用高斯朴素贝叶斯进行分类。\n",
    "\n",
    "```python\n",
    "# 假设特征呈正态分布\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(iris.data, iris.target)\n",
    "y_pred = gnb.predict(iris.data)\n",
    "accuracy_score(iris.target, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
