{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.79425\n",
    "\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/30538352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# 开始训练模型\n",
    "\n",
    "train_data_org = pd.read_csv('./input/train.csv')\n",
    "test_data_org = pd.read_csv('./input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 弃掉不需要的列\n",
    "def drop_col_not_req(df, cols):\n",
    "    df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立Fare Category\n",
    "def fare_category(fare):\n",
    "    if fare <= 4:\n",
    "        return 'Very_Low_Fare'\n",
    "    elif fare <= 10:\n",
    "        return 'Low_Fare'\n",
    "    elif fare <= 30:\n",
    "        return 'Med_Fare'\n",
    "    elif fare <= 45:\n",
    "        return 'High_Fare'\n",
    "    else:\n",
    "        return 'Very_High_Fare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立 PClass Fare Category\n",
    "def pclass_fare_category(df, Pclass_1_mean_fare, Pclass_2_mean_fare,\n",
    "                         Pclass_3_mean_fare):\n",
    "    if df['Pclass'] == 1:\n",
    "        if df['Fare'] <= Pclass_1_mean_fare:\n",
    "            return 'Pclass_1_Low_Fare'\n",
    "        else:\n",
    "            return 'Pclass_1_High_Fare'\n",
    "    elif df['Pclass'] == 2:\n",
    "        if df['Fare'] <= Pclass_2_mean_fare:\n",
    "            return 'Pclass_2_Low_Fare'\n",
    "        else:\n",
    "            return 'Pclass_2_High_Fare'\n",
    "    elif df['Pclass'] == 3:\n",
    "        if df['Fare'] <= Pclass_3_mean_fare:\n",
    "            return 'Pclass_3_Low_Fare'\n",
    "        else:\n",
    "            return 'Pclass_3_High_Fare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立Family Size Category\n",
    "def family_size_category(family_size):\n",
    "    if family_size <= 1:\n",
    "        return 'Single'\n",
    "    elif family_size <= 3:\n",
    "        return 'Small_Family'\n",
    "    else:\n",
    "        return 'Large_Family'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立Age_Group_category\n",
    "def age_group_category(age):\n",
    "    if age <= 1:\n",
    "        return 'Baby'\n",
    "    elif age <= 4:\n",
    "        return 'Toddler'\n",
    "    elif age <= 12:\n",
    "        return 'Child'\n",
    "    elif age <= 19:\n",
    "        return 'Teenager'\n",
    "    elif age <= 30:\n",
    "        return 'Adult'\n",
    "    elif age <= 50:\n",
    "        return 'Middle_Aged'\n",
    "    elif age < 60:\n",
    "        return 'Senior_Citizen'\n",
    "    else:\n",
    "        return 'Old'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立Name_Length_Category\n",
    "def name_length_category(name_len):\n",
    "    if name_len <= 19:\n",
    "        return 'Very_Short_Name'\n",
    "    elif name_len <= 28:\n",
    "        return 'Short_Name'\n",
    "    elif name_len <= 45:\n",
    "        return 'Medium_Name'\n",
    "    else:\n",
    "        return 'Long_Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 填充NaN值\n",
    "# 使用GradientBoostingRegressor和LinearRegression来填充Age值\n",
    "def fill_missing_age(missing_age_train, missing_age_test):\n",
    "    missing_age_X_train = missing_age_train.drop(['Age'], axis=1)\n",
    "    missing_age_Y_train = missing_age_train['Age']\n",
    "    missing_age_X_test = missing_age_test.drop(['Age'], axis=1)\n",
    "\n",
    "    gbm_reg = ensemble.GradientBoostingRegressor(random_state=42)\n",
    "    gbm_reg_param_grid = {'n_estimators': [2000], 'max_depth': [3],\n",
    "                          'learning_rate': [0.01], 'max_features': [3]}\n",
    "    gbm_reg_grid = model_selection.GridSearchCV(gbm_reg, gbm_reg_param_grid,\n",
    "                                                cv=10, n_jobs=25, verbose=1,\n",
    "                                                scoring='neg_mean_squared_error')\n",
    "    gbm_reg_grid.fit(missing_age_X_train, missing_age_Y_train)\n",
    "    print('Age feature Best GB Params:' + str(gbm_reg_grid.best_params_))\n",
    "    print('Age feature Best GB Score:' + str(gbm_reg_grid.best_score_))\n",
    "    print('GB Train Error for \"Age\" Feature Regressor:' + str(\n",
    "        gbm_reg_grid.score(missing_age_X_train, missing_age_Y_train)))\n",
    "    missing_age_test['Age_GB'] = gbm_reg_grid.predict(missing_age_X_test)\n",
    "    print(missing_age_test['Age_GB'][:4])\n",
    "\n",
    "    lrf_reg = LinearRegression()\n",
    "    lrf_reg_param_grid = {'fit_intercept': [True], 'normalize': [True]}\n",
    "    lrf_reg_grid = model_selection.GridSearchCV(lrf_reg, lrf_reg_param_grid,\n",
    "                                                cv=10, n_jobs=25, verbose=1,\n",
    "                                                scoring='neg_mean_squared_error')\n",
    "    lrf_reg_grid.fit(missing_age_X_train, missing_age_Y_train)\n",
    "    print('Age feature Best LR Params:' + str(lrf_reg_grid.best_params_))\n",
    "    print('Age feature Best LR Score:' + str(lrf_reg_grid.best_score_))\n",
    "    print('LR Train Error for \"Age\" Feature Regressor' + str(\n",
    "        lrf_reg_grid.score(missing_age_X_train, missing_age_Y_train)))\n",
    "\n",
    "    missing_age_test['Age_LRF'] = lrf_reg_grid.predict(missing_age_X_test)\n",
    "    print(missing_age_test['Age_LRF'][:4])\n",
    "\n",
    "    print('shape1', missing_age_test['Age'].shape, missing_age_test[['Age_GB', 'Age_LRF']].mode(axis=1).shape)\n",
    "    # missing_age_test['Age'] = missing_age_test[['Age_GB','Age_LRF']].mode(axis=1)\n",
    "    missing_age_test['Age'] = np.mean([missing_age_test['Age_GB'], missing_age_test['Age_LRF']])\n",
    "    print(missing_age_test['Age'][:4])\n",
    "    drop_col_not_req(missing_age_test, ['Age_GB', 'Age_LRF'])\n",
    "\n",
    "    return missing_age_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 筛选重要特征\n",
    "def get_top_n_features(titanic_train_data_X, titanic_train_data_Y, top_n_features):\n",
    "    # 随机森林\n",
    "    rf_est = RandomForestClassifier(random_state=42)\n",
    "    rf_param_grid = {'n_estimators': [500], 'min_samples_split': [2, 3], 'max_depth': [20]}\n",
    "    rf_grid = model_selection.GridSearchCV(rf_est, rf_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "    rf_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "\n",
    "    print('Top N Features Best RF Params:' + str(rf_grid.best_params_))\n",
    "    print('Top N Features Best RF Score:' + str(rf_grid.best_score_))\n",
    "    print('Top N Features RF Train Error:' + str(rf_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "    feature_imp_sorted_rf = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                          'importance': rf_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "        'importance', ascending=False)\n",
    "    features_top_n_rf = feature_imp_sorted_rf.head(top_n_features)['feature']\n",
    "    print('Sample 25 Features from RF Classifier')\n",
    "    print(str(features_top_n_rf[:25]))\n",
    "\n",
    "    # AdaBoost\n",
    "    ada_est = ensemble.AdaBoostClassifier(random_state=42)\n",
    "    ada_param_grid = {'n_estimators': [500], 'learning_rate': [0.5, 0.6]}\n",
    "    ada_grid = model_selection.GridSearchCV(ada_est, ada_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "    ada_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "\n",
    "    print('Top N Features Best Ada Params:' + str(ada_grid.best_params_))\n",
    "    print('Top N Features Best Ada Score:' + str(ada_grid.best_score_))\n",
    "    print('Top N Features Ada Train Error:' + str(ada_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "    feature_imp_sorted_ada = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                           'importance': ada_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "        'importance', ascending=False)\n",
    "    features_top_n_ada = feature_imp_sorted_ada.head(top_n_features)['feature']\n",
    "    print('Sample 25 Feature from Ada Classifier:')\n",
    "    print(str(features_top_n_ada[:25]))\n",
    "\n",
    "    # ExtraTree\n",
    "    et_est = ensemble.ExtraTreesClassifier(random_state=42)\n",
    "    et_param_grid = {'n_estimators': [500], 'min_samples_split': [3, 4], 'max_depth': [15]}\n",
    "    et_grid = model_selection.GridSearchCV(et_est, et_param_grid, n_jobs=25, cv=10, verbose=1)\n",
    "    et_grid.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "\n",
    "    print('Top N Features Best ET Params:' + str(et_grid.best_params_))\n",
    "    print('Top N Features Best ET Score:' + str(et_grid.best_score_))\n",
    "    print('Top N Features ET Train Error:' + str(et_grid.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "\n",
    "    feature_imp_sorted_et = pd.DataFrame({'feature': list(titanic_train_data_X),\n",
    "                                          'importance': et_grid.best_estimator_.feature_importances_}).sort_values(\n",
    "        'importance', ascending=False)\n",
    "    features_top_n_et = feature_imp_sorted_et.head(top_n_features)['feature']\n",
    "    print('Sample 25 Features from ET Classifier:')\n",
    "    print(str(features_top_n_et[:25]))\n",
    "\n",
    "    # 融合以上三个模型\n",
    "    features_top_n = pd.concat([features_top_n_rf, features_top_n_ada, features_top_n_et],\n",
    "                               ignore_index=True).drop_duplicates()\n",
    "\n",
    "    return features_top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liwei/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# 将训练集和测试集合并\n",
    "\n",
    "test_data_org['Survived'] = 0\n",
    "combined_train_test = train_data_org.append(test_data_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  10 | elapsed:    6.8s remaining:    6.8s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  10 | elapsed:    6.8s finished\n",
      "/Users/liwei/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age feature Best GB Params:{'learning_rate': 0.01, 'max_depth': 3, 'max_features': 3, 'n_estimators': 2000}\n",
      "Age feature Best GB Score:-112.69304020538209\n",
      "GB Train Error for \"Age\" Feature Regressor:-91.49364932164973\n",
      "5     33.521838\n",
      "17    33.281971\n",
      "19    33.045497\n",
      "26    26.564377\n",
      "Name: Age_GB, dtype: float64\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liwei/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  10 | elapsed:    1.3s remaining:    1.3s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  10 | elapsed:    1.4s finished\n",
      "/Users/liwei/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age feature Best LR Params:{'fit_intercept': True, 'normalize': True}\n",
      "Age feature Best LR Score:-6.602223435650571e+24\n",
      "LR Train Error for \"Age\" Feature Regressor-114.39079900961998\n",
      "5     34.5000\n",
      "17    33.6875\n",
      "19    31.0000\n",
      "26    26.8125\n",
      "Name: Age_LRF, dtype: float64\n",
      "shape1 (263,) (263, 2)\n",
      "5     29.543635\n",
      "17    29.543635\n",
      "19    29.543635\n",
      "26    29.543635\n",
      "Name: Age, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liwei/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/liwei/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/liwei/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Age          Fare        Parch       Pclass        SibSp  \\\n",
      "count  1.309000e+03  1.309000e+03  1309.000000  1309.000000  1309.000000   \n",
      "mean  -3.300984e-16 -1.302752e-16     0.385027     2.294882     0.498854   \n",
      "std    1.000382e+00  1.000382e+00     0.865560     0.837836     1.041658   \n",
      "min   -2.301683e+00 -8.759798e-01     0.000000     1.000000     0.000000   \n",
      "25%   -6.066728e-01 -5.435530e-01     0.000000     2.000000     0.000000   \n",
      "50%   -2.094051e-02 -4.967240e-01     0.000000     3.000000     0.000000   \n",
      "75%    4.027238e-01  2.374692e-03     0.000000     3.000000     1.000000   \n",
      "max    3.896789e+00  8.360045e+00     9.000000     3.000000     8.000000   \n",
      "\n",
      "          Survived   Embarked_C   Embarked_Q   Embarked_S   Sex_female  \\\n",
      "count  1309.000000  1309.000000  1309.000000  1309.000000  1309.000000   \n",
      "mean      0.261268     0.206264     0.093965     0.699771     0.355997   \n",
      "std       0.439494     0.404777     0.291891     0.458533     0.478997   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     1.000000     0.000000   \n",
      "75%       1.000000     0.000000     0.000000     1.000000     1.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "            ...           Cabin_G6      Cabin_T  Cabin_Letter_A  \\\n",
      "count       ...        1309.000000  1309.000000     1309.000000   \n",
      "mean        ...           0.003820     0.000764        0.016807   \n",
      "std         ...           0.061709     0.027639        0.128596   \n",
      "min         ...           0.000000     0.000000        0.000000   \n",
      "25%         ...           0.000000     0.000000        0.000000   \n",
      "50%         ...           0.000000     0.000000        0.000000   \n",
      "75%         ...           0.000000     0.000000        0.000000   \n",
      "max         ...           1.000000     1.000000        1.000000   \n",
      "\n",
      "       Cabin_Letter_B  Cabin_Letter_C  Cabin_Letter_D  Cabin_Letter_E  \\\n",
      "count     1309.000000     1309.000000     1309.000000     1309.000000   \n",
      "mean         0.049656        0.071811        0.035141        0.031322   \n",
      "std          0.217317        0.258273        0.184207        0.174252   \n",
      "min          0.000000        0.000000        0.000000        0.000000   \n",
      "25%          0.000000        0.000000        0.000000        0.000000   \n",
      "50%          0.000000        0.000000        0.000000        0.000000   \n",
      "75%          0.000000        0.000000        0.000000        0.000000   \n",
      "max          1.000000        1.000000        1.000000        1.000000   \n",
      "\n",
      "       Cabin_Letter_F  Cabin_Letter_G  Cabin_Letter_T  \n",
      "count     1309.000000     1309.000000     1309.000000  \n",
      "mean         0.016043        0.003820        0.000764  \n",
      "std          0.125688        0.061709        0.027639  \n",
      "min          0.000000        0.000000        0.000000  \n",
      "25%          0.000000        0.000000        0.000000  \n",
      "50%          0.000000        0.000000        0.000000  \n",
      "75%          0.000000        0.000000        0.000000  \n",
      "max          1.000000        1.000000        1.000000  \n",
      "\n",
      "[8 rows x 2309 columns]\n"
     ]
    }
   ],
   "source": [
    "# 特征工程\n",
    "\n",
    "# 1. Embarked\n",
    "# Embarkde中有两个NaN\n",
    "if combined_train_test['Embarked'].isnull().sum() != 0:\n",
    "    combined_train_test['Embarked'].fillna(combined_train_test['Embarked'].mode().iloc[0], inplace=True)\n",
    "\n",
    "emb_dummies_df = pd.get_dummies(combined_train_test['Embarked'],\n",
    "                                prefix=combined_train_test[['Embarked']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, emb_dummies_df], axis=1)\n",
    "\n",
    "# 2. Sex\n",
    "sex_dummies_df = pd.get_dummies(combined_train_test['Sex'], prefix=combined_train_test[['Sex']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, sex_dummies_df], axis=1)\n",
    "\n",
    "# 3. Name\n",
    "# Title\n",
    "combined_train_test['Title'] = combined_train_test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "# print(combined_train_test['Title'].groupby(by=combined_train_test['Title']).count().sort_values(ascending=False))\n",
    "\n",
    "title_Dict = {}\n",
    "title_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\n",
    "title_Dict.update(dict.fromkeys(['Jonkheer', 'Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))\n",
    "title_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))\n",
    "title_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))\n",
    "title_Dict.update(dict.fromkeys(['Mr'], 'Mr'))\n",
    "title_Dict.update(dict.fromkeys(['Master'], 'Master'))\n",
    "\n",
    "combined_train_test['Title'] = combined_train_test['Title'].map(title_Dict)\n",
    "# print(combined_train_test['Title'].groupby(by=combined_train_test['Title']).count().sort_values(ascending=False))\n",
    "\n",
    "title_dummies_df = pd.get_dummies(combined_train_test['Title'], prefix=combined_train_test[['Title']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, title_dummies_df], axis=1)\n",
    "# print(combined_train_test)\n",
    "\n",
    "# Name Length\n",
    "combined_train_test['Name_Length'] = combined_train_test['Name'].str.len()\n",
    "combined_train_test['Name_length_Category'] = combined_train_test['Name_Length'].map(name_length_category)\n",
    "# print(combined_train_test['Name_length_Category'].groupby(by=combined_train_test['Name_length_Category']).count().sort_values(ascending=False))\n",
    "\n",
    "le_name = LabelEncoder()\n",
    "le_name.fit(np.array(['Very_Short_Name', 'Short_Name', 'Medium_Name', 'Long_Name']))\n",
    "combined_train_test['Name_length_Category'] = le_name.transform(combined_train_test['Name_length_Category'])\n",
    "\n",
    "# print(combined_train_test[['Name_length_Category','Survived']].corr())\n",
    "name_length_dummies_df = pd.get_dummies(combined_train_test['Name_length_Category'],\n",
    "                                        prefix=combined_train_test[['Name_length_Category']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, name_length_dummies_df], axis=1)\n",
    "# print(combined_train_test)\n",
    "\n",
    "# First Name\n",
    "combined_train_test['First_Name'] = combined_train_test['Name'].str.extract('^(.+?),')\n",
    "first_name_dummies_df = pd.get_dummies(combined_train_test['First_Name'],\n",
    "                                       prefix=combined_train_test[['First_Name']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, first_name_dummies_df], axis=1)\n",
    "# print(combined_train_test)\n",
    "\n",
    "# Last_Name\n",
    "combined_train_test['Last_Name'] = combined_train_test['Name'].str.split('\\.')\n",
    "combined_train_test['Last_Name'] = combined_train_test['Last_Name'].str.strip('\\([^)]*\\)')\n",
    "combined_train_test['Last_Name'].fillna(combined_train_test['Name'].str.split('\\.').str[1])\n",
    "# print(combined_train_test['Last_Name'].groupby(by = combined_train_test['Last_Name']).count().sort_values(ascending = False)[:5])\n",
    "last_name_dummies_df = pd.get_dummies(combined_train_test['Last_Name'],\n",
    "                                      prefix=combined_train_test[['Last_Name']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, last_name_dummies_df], axis=1)\n",
    "\n",
    "# print(combined_train_test)\n",
    "\n",
    "# Original_Name\n",
    "combined_train_test['Original_Name'] = combined_train_test['Name'].str.split('\\((.*?)\\)').str[1].str.strip(\n",
    "    '\\\"').str.strip()\n",
    "# print(combined_train_test['Original_Name'].groupby(by = combined_train_test['Original_Name']).count().sort_values(ascending = False)[:5])\n",
    "original_name_dummies_df = pd.get_dummies(combined_train_test['Original_Name'],\n",
    "                                          prefix=combined_train_test[['Original_Name']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, original_name_dummies_df], axis=1)\n",
    "\n",
    "# 4. Fare\n",
    "# 填充NaN\n",
    "if combined_train_test['Fare'].isnull().sum() != 0:\n",
    "    combined_train_test['Fare'] = combined_train_test[['Fare']].fillna(\n",
    "        combined_train_test.groupby('Pclass').transform('mean'))\n",
    "\n",
    "# 将多人船票的价格平均到每人\n",
    "combined_train_test['Group_Ticket'] = combined_train_test['Fare'].groupby(\n",
    "    by=combined_train_test['Ticket']).transform('count')\n",
    "combined_train_test['Fare'] = combined_train_test['Fare'] / combined_train_test['Group_Ticket']\n",
    "combined_train_test.drop(['Group_Ticket'], axis=1, inplace=True)\n",
    "\n",
    "# 去除Fare为0的项\n",
    "if sum(n == 0 for n in combined_train_test.Fare.values.flatten()) > 0:\n",
    "    combined_train_test.loc[combined_train_test.Fare == 0, 'Fare'] = np.nan\n",
    "    combined_train_test['Fare'] = combined_train_test[['Fare']].fillna(\n",
    "        combined_train_test.groupby('Pclass').transform('mean'))\n",
    "# print(combined_train_test['Fare'].describe())\n",
    "# 建立Fare Category\n",
    "combined_train_test['Fare_Category'] = combined_train_test['Fare'].map(fare_category)\n",
    "le_fare = LabelEncoder()\n",
    "le_fare.fit(np.array(['Very_Low_Fare', 'Low_Fare', 'Med_Fare', 'High_Fare', 'Very_High_Fare']))\n",
    "combined_train_test['Fare_Category'] = le_fare.transform(combined_train_test['Fare_Category'])\n",
    "\n",
    "fare_cat_dummies_df = pd.get_dummies(combined_train_test['Fare_Category'],\n",
    "                                     prefix=combined_train_test[['Fare_Category']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, fare_cat_dummies_df], axis=1)\n",
    "# print(combined_train_test['Fare_Category'].groupby(by = combined_train_test['Fare_Category']).count().sort_values(ascending = False))\n",
    "\n",
    "# 5. Pclass\n",
    "# print(combined_train_test['Fare'].groupby(by = combined_train_test['Pclass']).mean())\n",
    "Pclass_1_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([1]).values[0]\n",
    "Pclass_2_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([2]).values[0]\n",
    "Pclass_3_mean_fare = combined_train_test['Fare'].groupby(by=combined_train_test['Pclass']).mean().get([3]).values[0]\n",
    "# 建立Pclass_Fare Category\n",
    "combined_train_test['Pclass_Fare_Category'] = combined_train_test.apply(pclass_fare_category, args=(\n",
    "    Pclass_1_mean_fare, Pclass_2_mean_fare, Pclass_3_mean_fare), axis=1)\n",
    "# print(combined_train_test['Pclass_Fare_Category'].groupby(by = combined_train_test['Pclass_Fare_Category']).count().sort_values(ascending = False))\n",
    "p_fare = LabelEncoder()\n",
    "p_fare.fit(np.array(\n",
    "    ['Pclass_1_Low_Fare', 'Pclass_1_High_Fare', 'Pclass_2_Low_Fare', 'Pclass_2_High_Fare', 'Pclass_3_Low_Fare',\n",
    "     'Pclass_3_High_Fare']))\n",
    "combined_train_test['Pclass_Fare_Category'] = p_fare.transform(combined_train_test['Pclass_Fare_Category'])\n",
    "\n",
    "# 6. Parch and SibSp\n",
    "\n",
    "combined_train_test['Family_Size'] = combined_train_test['Parch'] + combined_train_test['SibSp'] + 1\n",
    "combined_train_test['Family_Size_Category'] = combined_train_test['Family_Size'].map(family_size_category)\n",
    "\n",
    "le_family = LabelEncoder()\n",
    "le_family.fit(np.array(['Single', 'Small_Family', 'Large_Family']))\n",
    "combined_train_test['Family_Size_Category'] = le_family.transform(combined_train_test['Family_Size_Category'])\n",
    "\n",
    "fam_size_cat_dummies_df = pd.get_dummies(combined_train_test['Family_Size_Category'],\n",
    "                                         prefix=combined_train_test[['Family_Size_Category']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, fam_size_cat_dummies_df], axis=1)\n",
    "# print(combined_train_test)\n",
    "\n",
    "# 7. Age\n",
    "# 填充Age中的NaN值\n",
    "\n",
    "combined_train_test['Age_Null'] = combined_train_test['Age'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "\n",
    "missing_age_df = pd.DataFrame(\n",
    "    combined_train_test[['Age', 'Parch', 'Sex', 'SibSp', 'Family_Size', 'Family_Size_Category',\n",
    "                         'Title', 'Fare', 'Fare_Category', 'Pclass', 'Embarked']])\n",
    "missing_age_df = pd.get_dummies(missing_age_df,\n",
    "                                columns=['Title', 'Family_Size_Category', 'Fare_Category', 'Sex', 'Pclass',\n",
    "                                         'Embarked'])\n",
    "missing_age_train = missing_age_df[missing_age_df['Age'].notnull()]\n",
    "missing_age_test = missing_age_df[missing_age_df['Age'].isnull()]\n",
    "combined_train_test.loc[(combined_train_test.Age.isnull()), 'Age'] = fill_missing_age(missing_age_train,\n",
    "                                                                                      missing_age_test)\n",
    "# print(combined_train_test.describe())\n",
    "\n",
    "# 检查是否有异常值\n",
    "if sum(n < 0 for n in combined_train_test.Age.values.flatten()) > 0:\n",
    "    combined_train_test.loc[combined_train_test.Age < 0, 'Age'] = np.nan\n",
    "    combined_train_test['Age'] = combined_train_test[['Age']].fillna(\n",
    "        combined_train_test.groupby('Title').transform('mean'))\n",
    "# print(combined_train_test['Age'].groupby(by=combined_train_test['Title']).mean().sort_values(ascending=True))\n",
    "\n",
    "# 建立Age_Category\n",
    "combined_train_test['Age_Category'] = combined_train_test['Age'].map(age_group_category)\n",
    "le_age = LabelEncoder()\n",
    "le_age.fit(np.array(['Baby', 'Toddler', 'Child', 'Teenager', 'Adult', 'Middle_Aged', 'Senior_Citizen', 'Old']))\n",
    "combined_train_test['Age_Category'] = le_age.transform(combined_train_test['Age_Category'])\n",
    "age_cat_dummies_df = pd.get_dummies(combined_train_test['Age_Category'],\n",
    "                                    prefix=combined_train_test[['Age_Category']].columns[0])\n",
    "combined_train_test = pd.concat([combined_train_test, age_cat_dummies_df], axis=1)\n",
    "\n",
    "# 8. Ticket\n",
    "combined_train_test['Ticket_Letter'] = combined_train_test['Ticket'].str.split().str[0]\n",
    "combined_train_test['Ticket_Letter'] = combined_train_test['Ticket_Letter'].apply(\n",
    "    lambda x: np.nan if x.isnumeric() else x)\n",
    "combined_train_test['Ticket_Number'] = combined_train_test['Ticket'].apply(\n",
    "    lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "combined_train_test['Ticket_Number'].fillna(0, inplace=True)\n",
    "combined_train_test = pd.get_dummies(combined_train_test, columns=['Ticket', 'Ticket_Letter'])\n",
    "# print(combined_train_test.shape)\n",
    "\n",
    "# 9. Cabin\n",
    "combined_train_test['Cabin_Letter'] = combined_train_test['Cabin'].apply(\n",
    "    lambda x: str(x)[0] if pd.notnull(x) else x)\n",
    "combined_train_test = pd.get_dummies(combined_train_test, columns=['Cabin', 'Cabin_Letter'])\n",
    "# print(combined_train_test.shape)\n",
    "\n",
    "# 10. 将Age和Fare正则化\n",
    "scale_age_fare = preprocessing.StandardScaler().fit(combined_train_test[['Age', 'Fare']])\n",
    "combined_train_test[['Age', 'Fare']] = scale_age_fare.transform(combined_train_test[['Age', 'Fare']])\n",
    "\n",
    "# 11. 弃掉无用列\n",
    "combined_train_test.drop(['Name', 'PassengerId', 'Embarked', 'Sex', 'Title', 'Fare_Category',\n",
    "                          'Family_Size_Category', 'Age_Category', 'First_Name', 'Last_Name',\n",
    "                          'Original_Name', 'Name_length_Category'], axis=1, inplace=True)\n",
    "print(combined_train_test.describe())\n",
    "# 12. 整理数据\n",
    "\n",
    "train_data = combined_train_test[:891]\n",
    "test_data = combined_train_test[891:]\n",
    "\n",
    "titanic_train_data_X = train_data.drop(['Survived'], axis=1)\n",
    "titanic_train_data_Y = train_data['Survived']\n",
    "\n",
    "titanic_test_data_X = test_data.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  20 | elapsed:    7.9s remaining:    4.3s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  20 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N Features Best RF Params:{'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Top N Features Best RF Score:0.8327721661054994\n",
      "Top N Features RF Train Error:0.941638608305275\n",
      "Sample 25 Features from RF Classifier\n",
      "9                     Sex_male\n",
      "8                   Sex_female\n",
      "12                    Title_Mr\n",
      "16                 Name_Length\n",
      "1                         Fare\n",
      "11                  Title_Miss\n",
      "1134             Ticket_Number\n",
      "0                          Age\n",
      "1120      Pclass_Fare_Category\n",
      "13                   Title_Mrs\n",
      "3                       Pclass\n",
      "1121               Family_Size\n",
      "1116           Fare_Category_1\n",
      "4                        SibSp\n",
      "2                        Parch\n",
      "1124    Family_Size_Category_2\n",
      "18      Name_length_Category_1\n",
      "17      Name_length_Category_0\n",
      "1117           Fare_Category_2\n",
      "7                   Embarked_S\n",
      "20      Name_length_Category_3\n",
      "19      Name_length_Category_2\n",
      "1123    Family_Size_Category_1\n",
      "5                   Embarked_C\n",
      "10                Title_Master\n",
      "Name: feature, dtype: object\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  20 | elapsed:   17.0s remaining:    9.2s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  20 | elapsed:   21.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N Features Best Ada Params:{'learning_rate': 0.5, 'n_estimators': 500}\n",
      "Top N Features Best Ada Score:0.8484848484848485\n",
      "Top N Features Ada Train Error:0.9988776655443322\n",
      "Sample 25 Feature from Ada Classifier:\n",
      "1134             Ticket_Number\n",
      "1                         Fare\n",
      "0                          Age\n",
      "9                     Sex_male\n",
      "8                   Sex_female\n",
      "12                    Title_Mr\n",
      "16                 Name_Length\n",
      "10                Title_Master\n",
      "1239               Ticket_1601\n",
      "2106      Ticket_Letter_STON/O\n",
      "2193             Cabin_C22 C26\n",
      "37          First_Name_Allison\n",
      "5                   Embarked_C\n",
      "13                   Title_Mrs\n",
      "14               Title_Officer\n",
      "1120      Pclass_Fare_Category\n",
      "1115           Fare_Category_0\n",
      "2070        Ticket_Letter_A/5.\n",
      "1121               Family_Size\n",
      "1128            Age_Category_2\n",
      "409         First_Name_Jussila\n",
      "7                   Embarked_S\n",
      "4                        SibSp\n",
      "11                  Title_Miss\n",
      "18      Name_length_Category_1\n",
      "Name: feature, dtype: object\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  20 | elapsed:    5.5s remaining:    2.9s\n",
      "[Parallel(n_jobs=25)]: Done  20 out of  20 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N Features Best ET Params:{'max_depth': 15, 'min_samples_split': 3, 'n_estimators': 500}\n",
      "Top N Features Best ET Score:0.8316498316498316\n",
      "Top N Features ET Train Error:0.9169472502805837\n",
      "Sample 25 Features from ET Classifier:\n",
      "9                     Sex_male\n",
      "12                    Title_Mr\n",
      "8                   Sex_female\n",
      "11                  Title_Miss\n",
      "13                   Title_Mrs\n",
      "3                       Pclass\n",
      "1116           Fare_Category_1\n",
      "1120      Pclass_Fare_Category\n",
      "1124    Family_Size_Category_2\n",
      "17      Name_length_Category_0\n",
      "16                 Name_Length\n",
      "1117           Fare_Category_2\n",
      "1123    Family_Size_Category_1\n",
      "18      Name_length_Category_1\n",
      "1                         Fare\n",
      "20      Name_length_Category_3\n",
      "1121               Family_Size\n",
      "7                   Embarked_S\n",
      "2301            Cabin_Letter_B\n",
      "4                        SibSp\n",
      "10                Title_Master\n",
      "1122    Family_Size_Category_0\n",
      "19      Name_length_Category_2\n",
      "5                   Embarked_C\n",
      "2304            Cabin_Letter_E\n",
      "Name: feature, dtype: object\n",
      "Total Feature:(1309, 2309)\n",
      "Picked Feature(409,)\n"
     ]
    }
   ],
   "source": [
    "# 13. 利用特征值重要性排名来去除无用列\n",
    "feature_to_pick = 250\n",
    "feature_top_n = get_top_n_features(titanic_train_data_X, titanic_train_data_Y, feature_to_pick)\n",
    "print('Total Feature:' + str(combined_train_test.shape))\n",
    "print('Picked Feature' + str(feature_top_n.shape))\n",
    "\n",
    "titanic_train_data_X = titanic_train_data_X[feature_top_n]\n",
    "del titanic_train_data_X['Ticket_Number']\n",
    "titanic_test_data_X = titanic_test_data_X[feature_top_n]\n",
    "del titanic_test_data_X['Ticket_Number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier Score:0.9270482603815937\n",
      "VotingClassifier Estimators:[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=750, n_jobs=50,\n",
      "            oob_score=False, random_state=42, verbose=1, warm_start=False), GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.0008, loss='exponential', max_depth=3,\n",
      "              max_features='sqrt', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=2, min_samples_split=3,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=900,\n",
      "              n_iter_no_change=None, presort='auto', random_state=42,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=1, warm_start=False), ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=35, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=750, n_jobs=50,\n",
      "           oob_score=False, random_state=42, verbose=1, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "# 14.建立模型\n",
    "rf_est = ensemble.RandomForestClassifier(n_estimators=750, criterion='gini', max_features='sqrt',\n",
    "                                         max_depth=3, min_samples_split=4, min_samples_leaf=2,\n",
    "                                         n_jobs=50, random_state=42, verbose=1)\n",
    "gbm_est = ensemble.GradientBoostingClassifier(n_estimators=900, learning_rate=0.0008, loss='exponential',\n",
    "                                              min_samples_split=3, min_samples_leaf=2, max_features='sqrt',\n",
    "                                              max_depth=3, random_state=42, verbose=1)\n",
    "et_est = ensemble.ExtraTreesClassifier(n_estimators=750, max_features='sqrt', max_depth=35, n_jobs=50,\n",
    "                                       criterion='entropy', random_state=42, verbose=1)\n",
    "\n",
    "voting_est = ensemble.VotingClassifier(estimators=[('rf', rf_est), ('gbm', gbm_est), ('et', et_est)],\n",
    "                                       voting='soft', weights=[3, 5, 2],\n",
    "                                       n_jobs=50)\n",
    "voting_est.fit(titanic_train_data_X, titanic_train_data_Y)\n",
    "print('VotingClassifier Score:' + str(voting_est.score(titanic_train_data_X, titanic_train_data_Y)))\n",
    "print('VotingClassifier Estimators:' + str(voting_est.estimators_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test_data_X.drop(['Survived'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 750 out of 750 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "titanic_test_data_X['Survived'] = voting_est.predict(titanic_test_data_X)\n",
    "\n",
    "submission = pd.DataFrame({'PassengerId': test_data_org.loc[:, 'PassengerId'],\n",
    "                           'Survived': titanic_test_data_X.loc[:, 'Survived']})\n",
    "\n",
    "submission.to_csv('submission_result_notebook.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "# 0.79425\n",
    "# https://zhuanlan.zhihu.com/p/30538352\n",
    "\n",
    "    '''\n",
    "    要建立新的特征类目\n",
    "    1. Fare Category\n",
    "    2. Pclass Fare Category\n",
    "    3. Family Size Category\n",
    "    4. Age Group Category\n",
    "    5. Name Length Category\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
