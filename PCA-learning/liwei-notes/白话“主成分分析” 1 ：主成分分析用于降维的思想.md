# 白话“主成分分析” 1 ：主成分分析用于降维的思想

[TOC]

## 1. 什么是主成分分析

主成分分析，即分析“主成分”，找到“主成分”，英文是  Principal Component Analysis，简称 PCA。

经常和 PCA 一起出现的另一个词是降维（decomposition），当原始数据有成千甚至上万个特征的时候，降维可以节约空间，降低算法运行的开销，后面我们可以看到，PCA 的这种降维方式可以用于数据的可视化，还有去噪声的噪声。

在这里要说明的一点是：PCA 是方法，降维是思想，**PCA != 降维**， 降维还有很多方法，直接去掉一些无关的特征，也是降维，PCA 降维是线性降维，还有非线性降维，当然这里会扯出很多东西，不在我现有能解释的范围内。

本质上主成分分析是**找到一个欧式空间的线性变换，把原始数据从“一组旧的标准正交基下的表示”转化成“另一组新的标准正交基下的表示”，降维发生在新的标准正交基下的表示，直接去掉了后面几个维度的坐标值**。

这里我使用了一些“线性代数”中的概念和表述，但请你不要担心，全部在你的射程范围之内，主成分分析的思想其实并不深奥，只是被一些书籍用过于复杂的数学公式写得比较高大上，我倒觉得主成分分析是复习“线性代数”基础知识的绝佳材料。

## 2. PCA 应用于降维的思想

在初中，我们就学习过“从不同方向看物体”。PCA 应用于降维是想找到一个适合的方向看物体，去掉一些不太重要的特征，然后应用于建模。

请看下面这张图，左边是我斜着拿一支笔，三维空间中的一支笔，你在二维平面内你还能够看出是一支笔。如果我把这支笔像右边这张图那样展示给你，你有可能认为我拿着一个蓝色小球。

![从不同角度看一支笔](https://liweiwei1419.github.io/images/PAC/pca_02.jpg)

在三维空间里看到二维平面其实就是降维，降维以后的物体不是原来的物体，但可以作为原来物体的近似。

显而易见降维会带来损失，但**如果损失的信息并不是我们关注的主要的信息，其实并不妨碍我们对原本事物的认识**。再看上面的左图，这个角度比较完整地展现了一支笔的全貌，我们可以看出这是一只蓝色水笔，还有很多墨水没有用完，而右边的图，只从这支笔的底部看，虽然也是降维，但是丢失了很多信息，以至于我们有可能不知道这居然是一支笔。这说明**在我们有限的视野范围内观察一个事务，找对角度很关键**。北宋诗人苏轼有名诗句《题西林壁》传颂至今：

> 横看成岭侧成峰，
> 远近高低各不同。
> 不识庐山真面目，
> 只缘身在此山中。

说的就是如果你置身于庐山之中，便看不清庐山的全貌。

那么在 PCA 中，降维是如何发生的呢？请看下面这张图。

![先旋转再降维，损失信息更少.jpg](https://liweiwei1419.github.io/images/PAC/pca_03.jpg)

降维最直接的做法就是去掉一个维度的信息，这件事情就叫做朝着坐标轴“投影”，从左边这张图中，可以看出，投影到 $x$ 轴可能更好一些，因为投影到 $x$ 轴以后，数据更接近原始数据的样子。

而 PCA 用于降维的思想只是在这个基础上多做了一步：**先将坐标轴进行旋转，旋转到如右图所示，$x$ 轴尽量拟合原始数据的分布，然后将原始数据投影到 $x$ 轴上，很明显，这样得到的数据的近似表示比左边那张图要好，同样是二维降到一维，右边的方法损失的信息更少**。

再看看我前面拿笔的那两张图，右边的图之所以降维效果差，是因为我们选择的角度把蓝色水笔重要的维度给丢弃了。

还可以通过下面的例子来理解 PCA 降维的思想。

整数 $125$ 可以有以下两种分解方式：

- 第 1 种方式：$125 = 50 + 50 + 25$；
- 第 2 种方式：$125 = 100 + 20 + 5$；

分析：

- 第 1 种方式分解比较平均，随便去掉一个数，和分解之前（$125$）差别都比较大；
- 第 2 种方式不仅给出了分解，并且你很容易看出来，按照高位到低位分解，去掉 $5$ 剩下 $120$，去掉 $20$ 和 $5$ 剩下 $100$，越**是高位，与原始数值就越接近**。

依据矩阵分解的相关理论，更换以后的坐标系，从大到小包含了原始矩阵的信息，在降维的时候，我们就可以根据需要，保留前面的坐标轴，去掉尾巴的坐标轴，得到数据的降维表示。数据还是那个数据，维数减少了，但信息没有丢失多少，我们认为这是一件划算的事情。

整理一下，PCA 用于降维主要分为两步，第一步旋转坐标轴，第二步去掉一些特征，进而留下的特征可以作为原始数据的近似。在机器学习中，第一步可以认为是特征抽取或者是特征转换，而第二步可以认为是特征选择。即 “PCA = 特征抽取 + 特征选择”。

> 这里要说明一点：其实 PCA 变换坐标轴不一定是旋转，还有可能是对坐标系做了镜像，我们这里不妨只理解成 PCA 旋转了坐标轴到一个合适的位置即可。

## 3. 旋转以后的坐标轴应该具有什么性质

其实这个问题上面一部分已经回到了，即原始数据在旋转以后的坐标轴上进行投影更接近原始数据的分布。这样说还是有些抽象，二维三维空间还好理解，高维空间就不好理解了。如何用数学的表示描述这件事情呢？我们慢慢说。

### 3.1 机器学习中的数据使用二维矩阵表示，矩阵中的元素就是坐标

我们在做特征工程的时候，实际上就是在处理各种二维表格。举一个简单的例子，下面是一张成绩表，每一行是一个数据，一个数据组成的有序实数就是坐标，例如，表示张三的成绩，就可以用一个列向量 $(80,100,85)^T$ 来表示。

| 姓名 | 语文 | 数学  | 英语 |
| ---- | ---- | ----- | ---- |
| 张三 | $80$ | $100$ | $85$ |
| 李四 | $78$ | $90$  | $80$ |
| 王五 | $82$ | $70$  | $82$ |
| 赵六 | $79$ | $80$  | $87$ |

这是我们在机器学习中处理的数据。那么坐标是什么呢？**谈坐标一定不能离开坐标系**，我们在画这张表的时候，在心里就已经建立了一个坐标系，这个坐标系是约定速成的，以致于不用说明，那么这个人为建立的坐标系是什么呢？

### 3.2 谈论坐标不能离开基底

翻一翻《线性代数》教材，描述“坐标”的语言一定是“向量在什么什么基底下的坐标”。那么画出上面的成绩表的手，我们选用的基底是什么呢？
很简单，基底其实就是“语文成绩”、“数学成绩”、“英语成绩”。如果分别把“语文成绩”、“数学成绩”、“英语成绩”表示成 $x$ 轴、$y$ 轴、$z$ 轴，并且让它们都是单位向量，可以写成如下形式：
$$
\vec x=
\begin{bmatrix}1\\0\\0\\\end{bmatrix},
\vec y=
\begin{bmatrix}0\\1\\0\\\end{bmatrix},
\vec z=
\begin{bmatrix}0\\0\\1\\\end{bmatrix},
$$
那么张三的成绩 $(80,100,85)^T$ 可以写成：

$$
\begin{bmatrix}80\\100\\85\\\end{bmatrix} = 
80\begin{bmatrix}1\\0\\0\\\end{bmatrix}+ 
100\begin{bmatrix}0\\1\\0\\\end{bmatrix}+
85\begin{bmatrix}0\\0\\1\\\end{bmatrix},
$$

这里我的描述可能比较啰嗦，但我想你应该很容易明白我在说什么，因为这其实是我们大家都约定速成的事情。

谈到这里，很自然的一个问题是，如何找到一个合适的坐标系呢？即如何找到新的坐标系的各个坐标轴（即“基底”）呢？在这里我们要使用一些“线性代数”的概念和记号，不要担心，你都能想起来。

### 3.3 使用“线性代数”的语言描述 PCA 找坐标系的过程

为了说清楚后面的内容，这里我们会用一些“线性代数”的语言，如果你忘记了一些概念，能翻一翻书查一查资料是最好的，我再介绍一些概念的时候，可能会直接给出一些结论，不会展开了，因为再展开就是抄书了，你最好想一想或者翻翻书，但如果手边没有这样的条件，也可以选择性跳过一些内容。当然我会用尽量通俗的语言帮大家回忆起曾经你学习过的知识，其实它们非常有用。

我们知道坐标轴是基底，基底其实就是**一组向量**（不是一个向量），一般我们将每个向量单位化。取单位向量是因为将任意向量投影到单位向量的那个标量值好算，空间中的坐标就是向量**依次**投影到各个坐标轴的值，这个是是一个标量，带符号的。

这个结论我想是很显然，也容易理解。例如，$(80,100,85)^T$ 投影到 $(1,0,0)^T$ 就是 $85$，即 $(80,100,85)^T$ 的第 $1$ 个分量，注意 $(1,0,0)^T$ 是单位向量，才有这么好的性质，一般情况下的投影要根据投影公式计算。

再补充一句，我们选择一组向量成为基底，不仅仅希望它们都是单位向量，我们还希望这组基底里面的单位向量单位互相垂直，垂直也叫正交，此时，既是单位向量且相互正交的基底是**标准正交基**，上面的 $(1,0,0)^T$、$(0,1,0)^T$、$(0,0,1)^T$ 就是一组标准正交基，标准正交基有非常良好的性质：

> **如果使用标准正交基，向量在新的基底的坐标表示，就可以通过这个向量依次和标准正交基里的每一个基向量做内积，得到的标量组成向量，即是向量在新的基底的坐标表示。**

这个结论是非常重要的，如果忘记了的朋友们请一定翻一翻《线性代数》教材，描述“正交变换”的部分，简单来说就是**因为正交（垂直）的缘故，做内积的时候，不同的基向量的内积成为了 $0$，自己和自己做内积的时候，因为是单位向量，内积就是  $1$，标准正交基简化了坐标的计算**，经过后面的分析，我们可以看到，通过 PCA 找到的新的坐标系的基底恰好就是标准正交基。

谈到这里，你一定明白了，计算新坐标系下的坐标，其实就是计算原始坐标在新坐标轴下的投影，这个投影在新坐标轴是标准正交基的情况下，就成为了内积的计算。

在这里我们假设原始数据的坐标组成的矩阵是 $X$，$X$ 的每一行表示一个数据，$X$ 的每一列表示一个特征。原始数据在新坐标轴下的矩阵是 $Y$，同样 $Y$ 的每一行表示一个数据，$Y$ 的每一列表示一个特征。而假设新坐标轴的基底的每一个基向量**按列排成**矩阵 $P$。则矩阵 $X$、$Y$、$P$ 之间有如下关系：


$$
XP = Y
$$

我们知道矩阵乘法的定义是左边矩阵的每一行和右边矩阵的每一列分别相乘以后再相加，这就是内积的定义。通过上面的分析，我们知道 $P$ 是各个坐标轴，这些坐标轴是单位向量并且相互正交，计算到一个向量到它们的坐标，就是计算投影，计算投影就是计算内积。

到这里我们捋一捋，$X$ 是我们已知的，$P$ 是我们待求的，$X$、$P$ 知道了 $Y$ 就求出来了。如何求出 $P$ 呢？我们通过分析 $Y$ 应该满足的性质，进而就能得到 $P$ 了。

### 3.3.1 新坐标的各个维度方差依次减少

在线性回归中，我们知道，如果一个特征（即数据矩阵的一个列）在数值上都差不多，其实这一列对回归的结果没有什么帮助，它的系数几乎为 0，可以收缩到偏置项中。

因此，送入机器学习的算法的数据的特征，我们喜欢特征在数值有区分度，那么数值的区分度用什么度量呢？其实在中学我们就接触过这个概念了：方差。

方差表示了一组数据离散的程度，方差大是各种机器学习算法喜欢的样子，方差小的，表示这组数据在数值上差别不大，差别很小的时候作为特征就可以被忽略了。机器学习算法库 scikit-learn 就专门有用于特征选的类 [`sklearn.feature_selection.VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)，它的作用很简单，就是帮助我们移除方差小的特征。

在生活中也有这样的例子，差别不大的特征往往不是区分个体的主要因素。例如：女士在择偶的时候，往往会看重男士的家庭背景，拥有的资产多少。在男士的身高、体重、颜值要求上，可能只是这样的描述，身高不低于多少厘米，体重不超过多少公斤，颜值不要太低，年龄不要太大诸如此类。因为其实身高、体重、颜值，除了极个别的，全世界的成年人类放在一起比较，都不会差太多。但是家庭背景、知识、学识、见识、资产是可以有极端差别的，是具有区分度的。身高、体重、颜值在继承后代上带来的优势远不如“背景、知识、学识、见识、资产”能带给后代的多。

于是我们期望经过 PCA 以后，得到的原始数据在新坐标轴下的表示矩阵 $Y$ 具有这样的形式：$Y$ 的第一列方差最大，$Y$ 的第二列方差次之，以此类推，$Y$ 的最后一列方差最小。用投影的语言再描述一遍就是：原始数据在新坐标系的第一个坐标轴上的投影的方差最大，原始数据在新坐标系的第二个坐标轴上的投影的方差次之，依次类推。

### 3.3.2 新坐标的各个维度不能线性相关

还是从线性回归说起，如果两个特征线性相关，这个现象叫做“多重共线性”，一个特征可以用另一个特征线性表示，那么就可以去掉一个特征，即去掉冗余的特征。例如“年龄”和“出生年”，由年龄可以推出出生年，同样由出生年也可以推出年龄，“出生年”就可以被去掉。

那么两个特征之间的线性线性相关性用什么来衡量呢？这就是“协方差”。

第 1 点提到了方差，第 2 点提到了协方差，在数学上，有一个概念把它们联系在了一起，那就是“协方差矩阵”。

### 3.3.3 协方差矩阵

我们把上面第 1 点和第 2 点对矩阵 $Y$ 的要求写出来就是：

1. $Y$ 的各个列向量的方差逐渐减少；

2. $Y$ 的不同列向量线性无关。

把矩阵 $Y$ 按列分块，写成列向量的形式 $Y=(y_1,y_2,\cdots,y_n)$，这里每一个分量应该看出一个列向量，这样 $Y$ 才构成矩阵，我们可以如下这样构造一个矩阵：
$$
\cfrac{1}{m-1}Y^TY
$$
这里 $Y^T$ 是 $Y$ 的转置。如果你对这个矩阵感到陌生的话，你不妨动笔写一写：


$$
\cfrac{1}{m-1}
\begin{bmatrix}
y_1^T\\y_2^T\\ \vdots \\ y_n^T
\end{bmatrix}
\begin{bmatrix}
y_1,y_2,\cdots,y_n
\end{bmatrix}
=\cfrac{1}{m-1}\begin{bmatrix}
y_1^T y_1 & y_1^T y_2 & \cdots & y_1^T y_n \\
y_2^T y_1 & y_2^T y_2 & \cdots & y_2^T y_n \\
\vdots & \vdots & \ddots & \vdots \\
y_n^T y_1 & y_n^T y_2 & \cdots & y_n^T y_n
\end{bmatrix}
$$

可以看到，把前面的系数 $\cfrac{1}{m-1}$ 放进矩阵以后，这个矩阵主对角线上的元素，就是方差，非主对角线上的元素，就是不同列的协方差，并且还有一条很重要的性质：协方差矩阵是对称矩阵。

因此，我们把期望矩阵 $Y$ 满足的两个条件重新通过协方差矩阵 $\cfrac{1}{m-1}Y^TY$ 描述就是：

1. 协方差矩阵 $\cfrac{1}{m-1}Y^TY$ 主对角线上的元素越来越小；
2. 协方差矩阵 $\cfrac{1}{m-1}Y^TY$ 非主对角线上的元素为 $0$（非线性相关，即协方差为 $0$），即协方差矩阵 $\cfrac{1}{m-1}Y^TY$ 是对角矩阵。

由于

$$
\cfrac{1}{m-1}Y^TY
$$

是对角矩阵，又因为 $XP=Y$ ，代入上式消去 $Y$，即

$$
\begin{aligned}
\cfrac{1}{m-1}Y^TY &= \cfrac{1}{m-1}(XP)^T(XP)\\
&=\cfrac{1}{m-1}(P^TX^T)(XP)\\
&=\cfrac{1}{m-1}(P^T(X^TX)P)\\
&=P^T \left(\cfrac{1}{m-1}X^TX \right)P\\
\end{aligned}
$$
即我们希望
$$
P^T \left(\cfrac{1}{m-1}X^TX \right)P
$$
是对角矩阵。令 $A= \left(\cfrac{1}{m-1}X^TX \right)$，即可以定义 $A$ 是原始数据矩阵 $X$ 的协方差矩阵，注意到 $A$ 也是一个对称矩阵，那么问题就转化成的问题了，解决这个问题的使用的是在“线性代数”中是使用得非常广泛的“对称矩阵对角化”的相关结论。我们简述如下：

+ 实对称矩阵一定可以对角化；
+ 对一个对称矩阵对角化的步骤就是对这个对称矩阵进行特征值分解；

+ 实对称矩阵的各个特征向量相互正交，可以将实对称矩阵进行特征值分解得到的特征向量先正交化（施密特正交化）再单位化，得到的特征向量**按照特征值从大到小**、**按列排成**矩阵 $P$ ，这个矩阵就是正交矩阵，实对称矩阵总能够这样做，得到正交矩阵 $P$ 将 $A$ 对角化；
+ 正交矩阵 $P$ 满足 $P^{-1} = P^T$，因此我们要将数据恢复到原来的坐标系中，只需要乘以降维以后的 $P$ 矩阵的转置即可;
+ 正交矩阵表示的变换称之为正交变换，正交变换保持距离不变，保持内积（夹角）不变，因此正交变换施加于原始坐标系可以认为是对原始坐标系做了一次“旋转”或“镜像”。

$P$ 的第 1 列就是主成分 1 ，$P$ 的第 2 列就是主成分 2，以此类推。于是我们可以总结出 PCA 应用于降维的步骤。

## 4、PCA 应用于降维的步骤

1. 计算原始特征矩阵 $X$ 的协方差矩阵 $A=\left(\cfrac{1}{m-1}X^TX \right)$；

这里要特别说明的是，协方差和方差的定义中，都包括了减去均值这一操作，因此原始特征矩阵 $X$，我们要将其每一列减去每一列的均值。

2. 对协方差矩阵 $A$ 进行特征值分解；
3. 根据第 2 步得到的特征值和特征向量，按照特征值从大到小把对应的特征向量按列排成矩阵，去掉特征值小的最后几列，形成矩阵 $P$，$P$ 的前几列就是主成分；
4. 使用矩阵乘法 $XP$ 得到的矩阵 $Y$ 就是降维以后的矩阵。

