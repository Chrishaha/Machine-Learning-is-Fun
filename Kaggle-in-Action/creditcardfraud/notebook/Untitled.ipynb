{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../input/creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "count_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 269,
       "width": 399
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes.plot(kind='bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，数据是完全不平衡的！！\n",
    "\n",
    "这是一个清晰的例子，使用典型的准确度评分来评估我们的分类算法。例如，如果我们只是使用一个多数类为所有记录赋值，那么我们仍然具有很高的准确性，但是我们会错误地对所有“1”进行分类！！\n",
    "\n",
    "考虑到这种不平衡，有几种方法可以解决这一分类问题。\n",
    "\n",
    "收集更多数据？策略不错，但在这种情况下不适用\n",
    "\n",
    "更改性能指标：\n",
    "\n",
    "使用confusio nmatrix计算精度，调用\n",
    "\n",
    "F1分数（精确召回的加权平均值）\n",
    "\n",
    "使用kappa-这是一种分类精度，通过数据中类的不平衡进行归一化。\n",
    "\n",
    "ROC曲线-计算敏感性/特异性比率。\n",
    "\n",
    "重新采样数据集\n",
    "\n",
    "本质上，这是一种将数据处理成大约50-50比例的方法。\n",
    "\n",
    "实现这一点的一种方法是通过过度抽样，即添加表示不足的类的副本（当您只有很少的数据时更好）\n",
    "\n",
    "另一个是正在抽样，它从过度表示的类中删除实例（当他有大量数据时更好）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "途径\n",
    "\n",
    "我们不会首先执行功能工程。数据集已降级，以包含30个功能（28个非动态+时间+数量）。\n",
    "\n",
    "然后，我们将比较使用重新采样和不使用它时会发生什么。我们将使用一个简单的逻辑回归分类器来测试这种方法。\n",
    "\n",
    "我们将使用上面提到的一些性能指标来评估模型。\n",
    "\n",
    "我们将通过调整逻辑回归分类器中的参数，重复最佳重采样/非重采样方法。\n",
    "\n",
    "最后，我们将使用其他分类算法执行分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V20       V21       V22  \\\n",
       "0  0.098698  0.363787  0.090794     ...      0.251412 -0.018307  0.277838   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.069083 -0.225775 -0.638672   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.524980  0.247998  0.771679   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.208038 -0.108300  0.005274   \n",
       "4 -0.270533  0.817739  0.753074     ...      0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  normAmount  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053    0.244964  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   -0.342475  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752    1.160686  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458    0.140534  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   -0.073403  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(data[data.Class == 1])\n",
    "number_records_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "normal_indices = data[data.Class == 0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正例和负例比例一样多，进行建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(\n",
    "    normal_indices, number_records_fraud, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Appending the 2 indices\n",
    "# 下采样\n",
    "under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984, 30)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Under sample dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "under_sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  984\n"
     ]
    }
   ],
   "source": [
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \",\n",
    "      len(under_sample_data[under_sample_data.Class == 0]) /\n",
    "      len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \",\n",
    "      len(under_sample_data[under_sample_data.Class == 1]) /\n",
    "      len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \",\n",
    "      len(under_sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions train dataset:  199364\n",
      "Number transactions test dataset:  85443\n",
      "Total number of transactions:  284807\n",
      "\n",
      "Number transactions train dataset:  688\n",
      "Number transactions test dataset:  296\n",
      "Total number of transactions:  984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Whole dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"Number transactions train dataset: \", len(X_train))\n",
    "print(\"Number transactions test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train) + len(X_test))\n",
    "\n",
    "# Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(\n",
    "    X_undersample, y_undersample, test_size=0.3, random_state=0)\n",
    "print(\"\")\n",
    "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
    "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
    "print(\"Total number of transactions: \",\n",
    "      len(X_train_undersample) + len(X_test_undersample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 网格搜索\n",
    "def printing_Kfold_scores(x_train_data, y_train_data):\n",
    "    fold = KFold(len(y_train_data), 5, shuffle=False)\n",
    "\n",
    "    # Different C parameters\n",
    "    c_param_range = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "    results_table = pd.DataFrame(\n",
    "        index=range(len(c_param_range), 2),\n",
    "        columns=['C_parameter', 'Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('C parameter: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        for iteration, indices in enumerate(fold, start=1):\n",
    "\n",
    "            # Call the logistic regression model with a certain C parameter\n",
    "            lr = LogisticRegression(C=c_param, penalty='l1')\n",
    "\n",
    "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
    "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
    "            lr.fit(x_train_data.iloc[indices[0], :],\n",
    "                   y_train_data.iloc[indices[0], :].values.ravel())\n",
    "\n",
    "            # Predict values using the test indices in the training data\n",
    "            y_pred_undersample = lr.predict(\n",
    "                x_train_data.iloc[indices[1], :].values)\n",
    "\n",
    "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1], :].values,\n",
    "                                      y_pred_undersample)\n",
    "            recall_accs.append(recall_acc)\n",
    "            print('Iteration ', iteration, ': recall score = ', recall_acc)\n",
    "\n",
    "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
    "        results_table.ix[j, 'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('Mean recall score ', np.mean(recall_accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()][\n",
    "        'C_parameter']\n",
    "\n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print(\n",
    "        '*********************************************************************************'\n",
    "    )\n",
    "    print('Best model to choose from cross validation is with C parameter = ',\n",
    "          best_c)\n",
    "    print(\n",
    "        '*********************************************************************************'\n",
    "    )\n",
    "\n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "shuffle must be True or False; got 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9cd134e2d0f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprinting_Kfold_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_undersample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_undersample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-ce5f89609f5f>\u001b[0m in \u001b[0;36mprinting_Kfold_scores\u001b[0;34m(x_train_data, y_train_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprinting_Kfold_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Different C parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc_param_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liwei/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNSPLIT_WARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liwei/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             raise TypeError(\"shuffle must be True or False;\"\n\u001b[0;32m--> 293\u001b[0;31m                             \" got {0}\".format(shuffle))\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: shuffle must be True or False; got 5"
     ]
    }
   ],
   "source": [
    "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
