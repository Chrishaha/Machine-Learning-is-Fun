# “支持向量机”导读

我这篇笔记主要是介绍李航的《统计学习方法》第 7 章内容，以笔记的方式呈现。需要说明的是，为了使得本文尽量易懂，我把书上的一些标题或者专有名词做了更改，只是为了帮助初学的朋友们的理解。

## 学习“支持向量机”的步骤

“支持向量机”的内容非常多，掌握“支持向量机”是要花费一定时间的，经常看着看着，就不知道自己在看啥了。下面我简单罗列一下关于“支持向量机”的研究顺序，和各个知识点的地位和作用，希望能给看到我这篇笔记的朋友们一些帮助。

> "支持向量机"学习方法包含构建由简单到复杂的模型。
>
> —— 李航《统计学习方法》第 7 章“支持向量机”引言部分。



由简单到复杂的模型依次就是下面 3 个模型，我从统计学习 $3$ 要素的角度（模型、策略、方法，李航《统计学习方法》第 1 章就有介绍）对它们再做一层剖析：

### 第 1 阶段：严格线性可分支持向量机

+ “严格线性可分支持向量机”即书上说的“线性可分支持向量机”。我这里强调“严格”是为了突出我们此时对模型的假设，这个假设和感知机模型是完全一致的。

注：感知机模型是李航的《统计学习方法》介绍的第 1 个模型，麻雀虽小五脏俱全，我个人认为还是非常重要的，虽然很简单（那是在我了解他以后才觉得），在实际做分类的时候不会用它，但感知机模型从模型假设（线性可分）、到找到策略（损失函数最小化）、到使用方法（随机梯度下降法），比较完整清晰地体现了机器学习研究的基本步骤，并且感知机模型中提到的一些概念，例如线性可分、超平面、超平面的法向量、点到超平面的距离公式，定义类标为 $-1$ 和 $1$，使用类标乘以预测结果表示预测正确与否，这些思想和小技巧和我们的支持向量机可以说是完全通用的，甚至感知机也有对偶问题的算法（对偶问题的算法提出就是为了方便计算，将参数的学习归结为部分样本数据特征和标签的线性组合，二者在这点上也是非常一致的），这一点也是极其相似的，因此，在学习支持向量机的时候，感知机如果你忘了的话，可以趁机复习一下。

+ 后一个模型“差一点线性可分的支持向量机”就可以通过引入一个容忍错误的度量，和严格线性可分的情况使用相同的“策略”和“方法”完成分类任务；

具体，我们在看第 2 部分“线性支持向量机”的时候，其实可以看到，很多地方都是在摘抄第 1 部分的结论，在一些边界条件和目标函数上稍有不同而已，第 1 部分搞懂了，第 2 部分就非常快了。

+ 前面会跟你扯一些“代数间隔”、“几何间隔”的概念，我个人觉得不太重要，回过头来看都不要紧，介绍这两个概念无非是要保证后续叙述的无比正确性，这是写数学专著的要求；

+ 策略：策略这部分是很重要的，策略如果你忘了是什么，翻到书本第 1 章看看策略是什么，“硬间隔最大化”是核心思想，策略就是学习的准则，在“严格线性可分”的前提下，“硬间隔最大化”保证了模型有更好的泛化能力，即对未知数据的预测能力。

比较下面两张图，我们认为数据点应该尽量远离决策边界，这是我们指定严格线性可分支持向量机的学习准则，即策略，**因为如果数据点距离决策边界很近，决策边界的一点点改动，都会造成一些数据点的预测类别改变，这是不稳定的**。

说明：这里“硬间隔”对应了我前面说的“严格线性可分”，是相对于后面叙述的“软间隔”而言的，可以看到软间隔的时候，再来理解这一点。

- [ ] （这里要配图比较泛化能力好的和差的。）
+  我看到这一部分觉得比较难理解的是，怎么就把最后的目标函数，即两条间隔边界的距离变成 

$$
{\max_{w,b}} \quad \cfrac{1}{||w||}
$$

书上怎么就莫名其妙取 $\hat \gamma=1$，然后得到线性可分的支持向量机的最优化问题



$$
{\rm \min_{w,b}} \qquad \cfrac{1}{2}||w||^2 \\
{\rm s.t.} \qquad y_i(w \cdot x_i+b) - 1\geq 0, \quad i = 1, 2, \cdots,N
$$



不管是李航的《统计学习方法》还是周志华的《机器学习》，都只是使用缩放的方式解释了这件事情。我简单使用了变量代换的方式描述了这样做的合理性：

![](https://liweiwei1419.github.io/images/svm/svm_00.png)




看到这里，我们就明白了，”硬间隔最大化的支持向量机“问题最终转换成数学问题，我们解这个最优化问题：


$$
{\rm \min_{w,b}} \qquad \cfrac{1}{2}||w||^2 \\
{\rm s.t.} \qquad y_i(w \cdot x_i+b) - 1\geq 0, \quad i = 1, 2, \cdots,N
$$
输入是严格线性可分的数据对 $(\vec x,y)$，输出是这个问题的最优解 $w^*$ 和 $b^*$。然后最优超平面就得到了：

$$
w^* \cdot \vec x + b^* = 0
$$

做预测的时候把 $\vec x$ 代入 $w^* \cdot \vec x + b^*$ 判断符号，大于 $0$ 的归为一类，小于 $0$ 的归为另一类。

<b><font size='3' color='ff0000'>看到这里，其实我觉得可以先跳过书本的一些部分，直接到 7.2 节 ”线性支持向量机与软间隔最大化“。因为其实你已经知道”支持向量机“在做什么了。最起码它能解决严格线性可分的二分类数据问题。比起感知机而言，它得到的决策边界是唯一的，并且是对未知数据有很好的预测能力。
</font></b>

在这个问题里，模型和感知机是一样的，即二分类数据”严格“线性可分，策略是”硬间隔最大化“，这一点用数学表达成能表示成如下形式：

$$
{\rm \min_{w,b}} \qquad \cfrac{1}{2}||w||^2 \\
{\rm s.t.} \qquad y_i(w \cdot x_i+b) - 1\geq 0, \quad i = 1, 2, \cdots,N
$$

看到这里，你可能会问：

1、感知机只要分类都正确算法就停止了，它得到的决策边界是不唯一的；那么支持向量机得到的泛化性能比感知机好，你说它决策边界是唯一的，有什么证据吗？

答：当然有啦，就是书本上的定理 7.1，你可以回过头来再看它；

2、上面的最优化问题怎么解？

答：书本上 7.1.4 节”学习的对偶算法“就是在讲这一部分。这属于统计学习 $3$ 要素的角度（模型、策略、方法）中的方法。我们为了看清”支持向量机“的全貌，可以暂时略过，这部分内容不好消化，要结合附录一起看，刚开始看的时候会渐渐忘了这个算法到底是在做什么，跑得原来越远，拉不回来了，这是我刚开始看这部分内容的状态，聪明的你应该比我好。

3、只能解决严格线性可分的二分类问题，是不是太没用了。遇到不能线性可分的是不是就没辙了，多分类问题咋办？

答：之所以先讨论严格的情况，是因为线性可分好做。其它非线性可分的情况，可以转化成线性可分的情况来做。想一想从一元线性回归到多元线性回归，通过构造多项式特征，最后也是用线性回归的算法来做的，道理是一样的。至于使用二分类问题问题解决多分类问题，机器学习中使用的思想有 ”OvO“ 与 ”OvR“，很多书籍上都有介绍，在这里就不展开了。

写到这里，有一些概念我是没有提到，因为绝大多数将支持向量机的书籍都有讲，我罗列一下：支持向量、间隔、间隔边界、决策边界（即分离超平面），当然这个时候，你可以去看看函数间隔、几何间隔。这些概念其实都是见名知义的，如果你还比较模糊，不妨再翻翻书



## 第 2 阶段：差一点线性可分的支持向量机

有的时候，我们会遇到一些离群点，如果要照顾到它们分类的正确性是得不偿失的。我们不会”捡了芝麻丢了西瓜“，就是这个道理。

- [ ] 这里要配图，为了照顾到少数离群点，而使得泛化能力变差。

### 理解松弛变量

那么，此时我们是怎么做的呢？<b><font size='3' color='ff0000'>对每一个数据引入一个松弛变量，这个变量对于分类正确且在间隔边界之外的数据而言为 0 ，如果在间隔边界之内，即使分类正确，也会有一个很小的值。</font></b>

理解这个思想可以对比于逻辑回归，对于逻辑回归算法而言，不管你分类多正确，你的损失函数都会计入一个很小的概率。而此时支持向量机不是：

1、只要是分类正确的落在两个间隔边界之外的，我们认为是绝对安全的，所以不计损失；

2、即使分类正确，但是落在两个间隔边界之内的，我们认为此时的预测有一定风险，这个风险就量化为松弛变量的值；

3、分类错误的话，松弛变量的值就会大于 $1$ ，错得越离谱，这个值越大。

### 什么是惩罚系数 $C$ ？

$C$ 其实我们并不陌生，我们在学习岭回归和 LASSO 回归的时候，知道可以在损失函数的后面加上一个尾巴，这个尾巴表示了模型的复杂程度，这个尾巴前面有一个系数，用于平衡损失函数和模型的复杂度：

+ $C$ 很大，说明模型复杂度占据了损失函数的主要部分，因此真的”损失“那部分就会淡化了，因此模型会变得简单，容易欠拟合；
+ $C$ 很小，极端情况那就跟没有一样，此时模型容易变得复杂，容易过拟合。

这个道理在李航《统计学习方法》第 1 章介绍“策略”的部分，”结构风险=经验风险+正则化项“，正则化项又叫惩罚项，叫惩罚还好理解一点，不知道为什么要起这么拗口的名字，”风险“、”结构风险“、”经验风险“、”正则化“不查书真的不知道在说什么。

这里 $C$ 的作用是一样的，为了权衡”最大间隔“和”对每一个数据引入的松弛变量“这两件事。如果我们对那个越过边界的点特别在意，”宁可错杀三千，不能放过一个“，这个 $C$ 就要设置得很大，让这个离群点尽量不越界或者越界的距离短一点。如果我们想要顾大局，牺牲个体，就可以把 $C$ 设置小一点。

### 从松弛变量的角度理解 SVM 自带正则化

+ 引入了松弛变量以后，此时”差一点线性可分的支持向量机“所表达的最优化问题的数学表示式就跟”严格线性可分的支持向量机“差不多了，结构上一致，因此<b><font size='3' color='ff0000'>引入了松弛变量的线性支持向量机的问题就可以包括严格线性可分的支持向量机问题，因为其实只要引入的松弛变量都为 0，就是严格线性可分的支持向量机，两个问题合二为一了</font></b>；
+ 还有一点好处，对于支持向量机而言，我们从来不谈”支持向量机“的正则化，这其实是在这一节介绍的”差一点线性可分的支持向量机“自带的效果。

我们来看此时得到的数学表达式：

$$
{\rm \min_{w,b,\xi}} \qquad \cfrac{1}{2}||w||^2 + C\sum_{i=1}^N \xi_i\\
{\rm s.t.} \qquad y_i(w \cdot x_i+b) \geq 1 - \xi_i, \quad i = 1, 2, \cdots, N\\
\xi_i \ge0,\quad i = 1, 2, \cdots, N \quad\qquad
$$



目标函数从形式上看，前面的 $\cfrac{1}{2}||w||^2$  是不是就是我们学习过的岭回归的表示形式，而后面的 $\sum_{i=1}^N \xi_i$ 是针对所有样本而言的，$\xi_i$ 是针对每一个样本在指定的一个距离**间隔边界**（注意是间隔边界，不是决策边界）的距离，可以看成是一种损失。目标函数的形式不就是”损失 + 正则化“的样子吗？

+ 这里的松弛变量的值书本上又叫它”合页损失“。 

这部分内容虽然占据了一定篇幅，但是很多都是和第 1 阶段相同，很多结论无非就是在限制条件那里多了个 $C$。

## 第 3 阶段：线性不可分的支持向量机

终于到最后了。”和线性可分差距太远的时候“即这里对于”线性不可分的支持向量机“而言，我们的思路其实很简单，上面我们也提过，就是从”一元线性回归“到”多元线性回归“的思路，”通过构造特征升维“，”低维空间线性不可分，到高维空间就有可能线性可分“，李航老师的书上也用例子介绍了这个思路。

- [ ] 这里要配图。

### 直观理解”低维空间线性不可分，到高维空间就有可能线性可分“

这里要配图。

在这里我要特别强调一点：<b><font size='3' color='ff0000'>这一小节虽然一开始就在讲核技巧，整个章节也几乎都在介绍核技巧，但核技巧不是解决线性不可分的支持向量机问题的思想，它是一个具体的、可以操作的方法，核技巧是方法层面，不是思想层面，我们应该首先看到思想层面，因为它有时候更重要，而不应该被这个技巧、方法层面的东西蒙蔽了双眼。</font></b>

这里举一个可能不是很恰当的例子：一个学校来了两个老师，一个是普通二本师范院校刚刚毕业的学生 A，他是来工作的，一个是名牌大学的本科生 B，他来上课只是为了为考研赚一些生活费，他们的教学效果和受欢迎程度很可能是 A 更优，原因很简单：A 是来教书的，他选择了教育行业是热爱教学，经过 4 年的专业训练和学习，在教学技能和教学态度上很可能远远超过 B，而 B 有可能根本不喜欢教书，名牌大学毕业只是他的标签，他来教书只是为了贴补生活费，他的主要精力放在考研上。所以我们看待问题不能只看到这个问题吸引人的部分，有时要先看看它是用来干什么的。

<b><font size='3' color='ff0000'>因此我们就可以在高维空间中，使用第 1 阶段和第 2 阶段介绍的线性可分或者近似线性可分的策略和方法来解决在高维空间线性可分的问题，进而解决在原始低维空间中线性不可分的问题。核心思想就是构造特征，转化。</font></b>

其实到这里支持向量机的主要框架就介绍完了，就这么简单。那么你要问我”核技巧“被你吃了吗？不是的，<b><font size='3' color='ff0000'>核技巧只是我们为了解决在高维空间中线性可分问题的时候使用的一个方法，因为它有一定技巧性，还有一点点神奇、神秘的色彩，是一个可以让我们偷懒的”捷径“，所以我们叫它”核技巧“</font></b>。

### ”核技巧“是方法层面，而不是思想层面的东西

上面说了”线性不可分的支持向量机“首先是构造特征，把空间映射到高维空间，那么具体怎么做呢？

+ 首先”从低维空间到高维空间的映射“一定是非线性映射，才有可能把低维空间的超曲面”拉直“，问题来了，这个映射是什么？怎么找到这个映射？
+ 其实这个映射找到以后，我们要在高维空间中做运算，我们要意识到一点：在高维空间中做运算，其实是很消耗资源的。

单单第一件事情其实就很不好办了，这个非线性映射是什么，那么多非线性映射，你怎么找。好在这件事情，伟大的数学家（计算机科学家）帮我们解决了。核技巧就帮我们一下子干了这两件事，核技巧给出了一个具体的样子，它虽然没有直接给出非线性映射的样子，但其实我们并不关心，我们只关心那个在高维空间中进行运算的结果（具体可以等到理解了对偶问题的算法再来理解），核技巧把它们合二为一，具体化了，我们剩下要做的只是调参，如果你的数据很干净，噪声很小，你可以设置参数，使得算法预测非常准确，这样即使过拟合，影响也不大，如果你的数据噪声很大，就得设置另一套参数，避免过拟合，这正是核技巧神奇、神秘的地方。

”核技巧“把一个抽象的问题具体化，简单化，并且可以且容易操作。在这里还是强调一下，”核技巧“是方法层面，而不是思想层面的东西。处理非线性可分的问题，SVM 处理的思路是构造特征，使用样本在新的高维特征空间中线性可分。

如果我们能够有一种方法轻松地解决低维空间中的非线性可分问题，或者我们轻松地找到了从低维空间到高维空间的非线性映射，并且在高维空间中进行计算毫不费事，那可能核函数（核技巧）就不为我们所知了。



---

后面还没有介绍的内容有：

+ 如何求解第 1 部分提到的数学化以后的带约束的最优化问题；
+ 拉格朗日乘子法
+ 拉格朗日对偶问题
+ KKT 条件
+ 核函数

当然，绝大部分介绍 SVM 的书籍在这些知识上已经讲解得很正确了，我只会谈一些直观理解和应用的部分。





+ 仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据，效率高；
+ SVM 模型的目标函数计算了距离，距离是对特征缩放敏感的，这一点类似于在 k 近邻算法中计算距离。
+ SVM 用于回归；
+ SVM 如何用于异常检测。
+ 软间隔最大化：那些参数和是不是支持向量机的关系：利用对偶互补条件，介绍合页损失
+ 落在软间隔之内的样本也是支持向量



求解：拉格朗日乘子法

应用：最大熵模型，SVM，线性判别分析，主成分分析

