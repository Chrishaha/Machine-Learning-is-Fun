{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “k 近邻算法” 综述\n",
    "\n",
    "\n",
    "## 1. “k 近邻算法”用于分类问题的基本思想\n",
    "\n",
    "“k 近邻算法” 几乎是机器学习算法中最简单的算法，使用它用于分类的核心思想就是“物以类聚，人以群分”，即未标记样本的类别由距离其最近的 $k$ 个邻居投票来决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![图片来自周志华《机器学习》第 10 章第 1 节](./k 近邻算法思想示意图.jpg)\n",
    "说明：图片来自周志华《机器学习》第 10 章第 1 节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 从 “k 近邻算法” 开始理解机器学习的各种概念\n",
    "\n",
    "“k 近邻算法”因为在算法层面理解容易，因此可以从使用“k 近邻算法”处理分类问题着手，解释机器学习中的各种概念和习惯性做法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 认识有监督学习、分类学习、回归学习\n",
    "\n",
    "有监督学习的数据包含了“特征”和“标签”。根据这些数据对新的数据的分类进行预测或预测，如果待预测的目标变量是离散值，那么这一类问题称之为“分类问题”；如果待预测的目标变量是连续值，那么这一类问题称之为“回归问题”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 评估算法需要使用一部分带标签的数据，不能使用在训练过程中出现的数据用于评估算法\n",
    "\n",
    "这一点很像我们以前学习的时候，常常会买一本练习册做题，如果这本练习册没有参考答案，你就不知道自己做对与否。而**真正检验你学习水平的大型考试，例如期中考试、期末考试、中考、高考都是重新出题，如果使用以前出现过的题目，则不能检验你学习的真实水平，因为你有可能是记住了问题的解法，而没有理解它**。\n",
    "\n",
    "因此采集到的所有**带标签**的样本，应该分离一部分用于测试。那么评估算法应该采用什么指标呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 评估算法好坏的指标\n",
    "\n",
    "k 近邻算法用于分类问题使用的准确率作为指标。在数据分布不平衡的时候，就不能使用准确率了，而应该使用精准率、召回率、混淆矩阵等，甚至应该看看 auc。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 初步认识超参数、模型的复杂度、欠拟合、过拟合\n",
    "\n",
    "“k 近邻算法” 中的 $k$ 就是一个超参数，它决定了模型的复杂度。\n",
    "\n",
    "+ $k$ 越小，模型就越复杂。极端情况下 $k=1$ ，新来的预测数据的类别取决于训练样本中离他最近的那个样本的类别，如果这个样本恰好是标记错误的样本，预测就可能发生错误，因为它看不到更多数据，就有可能过拟合，学习到的不是样本数据的一般规律；\n",
    "+ $k$ 越大，模型就越简单，极端情况下 $k$ 等于所有训练样本的个数，此时每新来一个数据做预测的时候，就直接把训练样本中出现最多的类别数返回就行了，这样的模型过于简单，以致于失去了算法真正的意义，所有的预测数据都返回同一个值，对数据不能很好的预测，这是欠拟合。\n",
    "\n",
    "“k 近邻算法” 还有其它超参数，使用的距离的定义是欧氏距离还是闵式距离，闵式距离的参数 $p$ 是多少，投票的时候是“平票”还是“加权投票”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 网格搜索与交叉验证\n",
    "\n",
    "网格搜索其实就是暴力搜索，把我们认为可能合理的超参数和超参数的组合输入算法。而**评价一组超参数的好坏一定不能使用测试数据集，一般的做法是从训练数据集中再分离出一部分数据用于验证超参数的好坏，并且这种验证超参数好坏的做法要使用不同的训练数据集的子集重复多次，这就是交叉验证**；\n",
    "\n",
    "交叉验证用于选择超参数，由于分离数据集其实带有一定的随机性，把所有的数据集都看一遍，多次取平均的做法，比起一次性随机地使用训练数据集的一部分子集作为测试数据集要靠谱。\n",
    "\n",
    "网格搜索中就用到了交叉验证，通过被框架封装了起来，不用我们手动实现。\n",
    "\n",
    "\n",
    "### 2.5 数据标准化\n",
    "\n",
    "数据标准化是我刚开始接触学习机器学习算法的时候经常被忽略的。由于 k 近邻算法使用距离作为度量，数据在量纲上的统一是十分重要的，数据标准化可以避免计算出来的距离被量纲大的特征所主导。\n",
    "\n",
    "后面我们可以看到数据标准化在梯度下降中也发挥很大的作用，还有 SVM、K-means 这些距离度量的算法，都要求我们对数据进行标准化。\n",
    "\n",
    "\n",
    "例如《机器学习实战》提供的例子：\n",
    "\n",
    "![](./特征不在一个数量级上，要做标准化.jpg)\n",
    "\n",
    "\n",
    "+ <b><font size='3' color='ff0000'>在数据标准化这件事上，还要注意一点，训练数据集和测试数据集一定都使用训练数据集的标准化方式，请看下面的公式。</font></b>\n",
    "\n",
    "$$\n",
    "标准化的训练数据集 = \\cfrac{原始训练数据集数据-训练数据集的平均值}{训练数据集的标准差}\n",
    "$$\n",
    "\n",
    "$$\n",
    "标准化的测试数据集 = \\cfrac{原始训练测试集数据-训练数据集的平均值}{训练数据集的标准差}\n",
    "$$\n",
    "\n",
    "<b><font size='3' color='ff0000'>测试数据集在标准化的时候，一定也要使用“训练数据集的平均值”和“训练数据集的标准差”，而不能使用测试数据集的。</font></b>\n",
    "\n",
    "原因其实很简单：\n",
    "\n",
    "1、标准化其实可以视为算法的一部分，既然数据集都减去了一个数，然后除以一个数，这两个数对于所有的数据来说，就要一视同仁；  \n",
    "2、训练数据集其实很少，在预测新样本的时候，新样本就更少得可怜，如果新样本就一个数据，它的均值就是它自己，标准差是 0 ，这根本就不合理。  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 在二维平面上绘制决策边界\n",
    "\n",
    "方法很简单，就是在整个二维平面上采用一定密度的数据做预测，预测的不同类别使用不同的颜色标记，此时不同的类别之间就会形成决策边界。\n",
    "\n",
    "k 近邻算法的训练过程，即是利用训练数据集，**对特征向量空间进行划分**\n",
    "\n",
    "![李航《统计学习方法》P37](https://upload-images.jianshu.io/upload_images/414598-90d7e2f862ea411d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "说明：从这张图中，你可以看到决策边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k 近邻算法的三要素\n",
    "\n",
    "1. 超参数：$k$ ；\n",
    "\n",
    "2. 距离的定义（例如：欧氏距离）；\n",
    "\n",
    "3. 决策的规则（例如：投票表决，或者加权投票）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 手写 k 近邻算法的核心代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python3\n",
    "distances = [np.linalg.norm(point - X) for point in X_train]\n",
    "print(\"打印每个点距离待测点的距离：\")\n",
    "for index, distance in enumerate(distances):\n",
    "    print(\"[{}] {}\".format(index, np.round(distance, 2)))\n",
    "\n",
    "sorted_index = np.argsort(distances)\n",
    "print(y_train[sorted_index])\n",
    "\n",
    "k = 6\n",
    "topK = y_train[sorted_index][:k]\n",
    "print(topK)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "votes = Counter(topK)\n",
    "mc = votes.most_common(n=1)\n",
    "print(mc)\n",
    "print(\"根据投票得出的点 X 的标签为：\", mc[0][0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. $k$ 近邻算法的特点\n",
    "\n",
    "\n",
    "在整理这部分内容的时候，发现优点和缺点其实要在一定的前提下讨论，所以就干脆放在一起，说一说 k 近邻算法的特点。\n",
    "\n",
    "\n",
    "+ $k$ 近邻算法是一个懒惰学习的算法，没有显式的学习过程，即没有它没有训练的步骤，是一个基于记忆的学习算法；\n",
    "+ “多数表决”规则等价于“经验风险最小化”（我们的算法在训练数据集上“风险”最小）；\n",
    "+ $k$ 近邻算法的优化实现：kd 树，即是给训练数据建立树结构一样的索引，期望快速找到 $k$ 个邻居，以防止线性扫描。\n",
    "+ 对异常值和噪声有较高的容忍度，在算距离的时候，异常值和噪声离待预测的点的距离会比较远，且个数较少，就不会参与最终结果的投票； \n",
    "+ $k$ 近邻算法是一种在线学习技术，新数据可以直接加入数据集而不必进行重新训练，但与此同时在线学习计算量大，对内存的需求也较大。基本的 $k$ 近邻算法每预测一个“点”的分类都会重新进行一次**全局**运算，对于样本容量大的数据集计算量比较大；\n",
    "+ $k$ 近邻算法天生就支持多分类，类似还有决策树、朴素贝叶斯分类，它们区别于感知机、逻辑回归、SVM 这类原生只用于二分类问题的算法；\n",
    "+ **维度灾难**：在高维空间中计算距离的时候，就会变得非常远；\n",
    "+ 样本不平衡时，预测偏差会比较大；\n",
    "+ $k$ 值大小的选择得依靠经验或者交叉验证得到，不能自己拍脑门随便指定一个；\n",
    "+ 增加邻居的权重，距离越近，权重越高，参数：`weights`；\n",
    "\n",
    "维基百科[最近邻居法](https://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95)词条中是这样介绍的：\n",
    "\n",
    "> 无论是分类还是回归，衡量邻居的权重都非常有用，使较近邻居的权重比较远邻居的权重大。例如，一种常见的加权方案是给每个邻居权重赋值为 $\\cfrac{1}{d}$，其中 $d$ 是到邻居的距离。\n",
    "\n",
    "\n",
    "+ 使用一定半径内的点，当数据采样不均匀的时候，该算法可以取得更好的性能，可以参考 `from sklearn.neighbors import RadiusNeighborsClassifier`；\n",
    "\n",
    "+ $k$ 近邻算法还可以用于回归，找最近的邻居，计算它们的平均值就可以了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. $k$ 近邻算法的应用领域\n",
    "\n",
    "文本分类、模式识别、聚类分析，多分类领域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 参考资料\n",
    "\n",
    "+ 李航《统计学习方法》第 3 章：k 近邻法\n",
    "说明：介绍 kd 树，并给出了例子。\n",
    "\n",
    "\n",
    "+ 周志华《机器学习》第 10 章第 1 节\n",
    "说明：只简单介绍了思想，并给出了 k 近邻算法虽简单但预测效果在一定情况下比最优贝叶斯估计强的结论（我的这个概括待推敲），没有《统计学习方法》介绍详细。\n",
    "\n",
    "+ [美] Peter Harrington 著，李锐，李鹏，曲亚东 等 译《机器学习实战》第 2 章\n",
    "说明：想吐槽一下这本书在这一章节给出的示例代码，很简单的一个算法，它给出的代码变得很复杂，其实并不利于理解 k 近邻算法的基本思想。\n",
    "\n",
    "+ 理解 kd 树的文章：https://www.cnblogs.com/lesleysbw/p/6074662.html\n",
    "+ 理解 balltree 的文章：https://blog.csdn.net/pipisorry/article/details/53156836\n",
    "+ https://blog.csdn.net/skyline0623/article/details/8154911\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
